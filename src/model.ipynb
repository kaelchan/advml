{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-27 01:04:26\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    def __init__(self):\n",
    "        self.info = 'main'\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def start(self, info):\n",
    "        self.info = info\n",
    "        self.start_time = time.time()\n",
    "        self.checkpoint('start', elapsed_on=False)\n",
    "    \n",
    "    def end(self):\n",
    "        self.checkpoint(' end ')\n",
    "        \n",
    "    def checkpoint(self, tag, elapsed_on=True):\n",
    "        if elapsed_on:\n",
    "            elapsed = datetime.timedelta(seconds=round(time.time() - self.start_time))\n",
    "            expanded_info = self.info + ' [time elapsed: %s]' % str(elapsed)\n",
    "        else:\n",
    "            expanded_info = self.info\n",
    "        self.output(tag, info=expanded_info)\n",
    "        \n",
    "    def output(self, tag=' '*5, info=''):\n",
    "        if type(info) != type(''):\n",
    "            info = str(info)\n",
    "        print('[%s] (%s) %s' % (Timer.get_current_time(), tag, info))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_current_time():\n",
    "        return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "timer = Timer()\n",
    "sub_timer = Timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-04-27 01:05:03] (start) Load Data\n"
     ]
    }
   ],
   "source": [
    "timer.start('Load Data')\n",
    "# directory = '../data/split/'\n",
    "# df_train = pd.read_csv(directory + 'train.csv')\n",
    "# df_test_warm = pd.read_csv(directory + 'test_warm.csv')\n",
    "# df_test_cold_user = pd.read_csv(directory + 'test_cold_user.csv')\n",
    "# df_test_cold_item = pd.read_csv(directory + 'test_cold_item.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-04-27 01:05:10] (context) Load Data [time elapsed: 0:00:07]\n"
     ]
    }
   ],
   "source": [
    "directory = '../data/context/'\n",
    "df_event_context = pd.read_csv(directory + 'event_context.csv')\n",
    "df_song_context = pd.read_csv(directory + 'song_context.csv')\n",
    "df_user_context = pd.read_csv(directory + 'user_context.csv')\n",
    "timer.checkpoint('context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_user = len(df_user_context.user_id.unique())\n",
    "num_item = len(df_song_context.song_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load target sets\n",
    "# import pickle\n",
    "# with open('../data/split/target_set.pickle', 'rb') as handle:\n",
    "#     target_set = pickle.load(handle)\n",
    "# with open('../data/split/train_target_set.pickle', 'rb') as handle:\n",
    "#     train_target_set = pickle.load(handle)\n",
    "# with open('../data/split/test_warm_target_set.pickle', 'rb') as handle:\n",
    "#     test_warm_target_set = pickle.load(handle)\n",
    "# with open('../data/split/test_cold_user_target_set.pickle', 'rb') as handle:\n",
    "#     test_cold_user_target_set = pickle.load(handle)\n",
    "# with open('../data/split/test_cold_item_target_set.pickle', 'rb') as handle:\n",
    "#     test_cold_item_target_set = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, name):\n",
    "        '''\n",
    "        user_list: list(int), the list of user id's used in the dataset\n",
    "        target_set: list(set), set of target items for each user\n",
    "        item_list: list(numpy array), list of items used in the dataset for each user\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.df = None\n",
    "        self.user_list = None\n",
    "        self.item_list = None\n",
    "        self.target_set = None\n",
    "    \n",
    "    def load(self, filename):\n",
    "        self.df = pd.read_csv(filename)\n",
    "        # prepare user list\n",
    "        self.user_list = self.df['user_id'].unique()\n",
    "        \n",
    "        # prepare item list\n",
    "        self.item_list = [[] for i in range(num_user)]\n",
    "        self.df.apply(\n",
    "            lambda row: self.item_list[row['user_id']].append(row['song_id']),\n",
    "            axis=1\n",
    "        )\n",
    "        self.item_list = list(map(np.array, self.item_list))\n",
    "        \n",
    "        # prepare target set\n",
    "        self.target_set = [set() for i in range(num_user)]\n",
    "        self.df[self.df['target'] == 1].apply(\n",
    "            lambda row: self.target_set[row['user_id']].add(row['song_id']),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "# def load_split(name):\n",
    "#     data = Data(name)\n",
    "#     # load the user ids in the data set\n",
    "#     with open('../data/split/' + name + '_user_list.pickle', 'rb') as handle:\n",
    "#         data.user_list = pickle.load(handle)\n",
    "        \n",
    "#     # load the list(set) for items in the data set with label=1\n",
    "#     with open('../data/split/' + name + '_target_set.pickle', 'rb') as handle:\n",
    "#         data.target_set = pickle.load(handle)\n",
    "        \n",
    "#     # load the list(set) for all items in the data set\n",
    "#     with open('../data/split/' + name + '_item_set.pickle', 'rb') as handle:\n",
    "#         data.item_set = pickle.load(handle)\n",
    "        \n",
    "#     return data\n",
    "\n",
    "def load_split(name):\n",
    "    directory = '../data/split/'\n",
    "    data = Data(name)\n",
    "    data.load(directory + name + '.csv')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-04-27 01:09:30] ( end ) Load Data [time elapsed: 0:04:27]\n"
     ]
    }
   ],
   "source": [
    "# data_train = load_split('train')\n",
    "# data_test_warm = load_split('test_warm')\n",
    "# data_test_cold_user = load_split('test_cold_user')\n",
    "# data_test_cold_item = load_split('test_cold_item')\n",
    "\n",
    "data_train = load_split('train')\n",
    "data_test_warm = load_split('test_warm')\n",
    "data_test_cold_user = load_split('test_cold_user')\n",
    "data_test_cold_item = load_split('test_cold_item')\n",
    "timer.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: MF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Reshape, Lambda\n",
    "from keras.layers.merge import concatenate, dot\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import RandomUniform, RandomNormal, TruncatedNormal, Zeros\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define MF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "REG_LAMBDA = 0\n",
    "EMBED_DIM = 64\n",
    "\n",
    "vocab_size = num_user\n",
    "user_embeddings = Embedding(\n",
    "    input_dim = vocab_size,\n",
    "    output_dim = EMBED_DIM,\n",
    "    embeddings_initializer = RandomUniform(minval=-0.1, maxval=0.1),\n",
    "    embeddings_regularizer = l2(REG_LAMBDA),\n",
    "    input_length = 1,\n",
    "    name = 'user_embed',\n",
    "    trainable=True)\n",
    "\n",
    "vocab_size = num_item\n",
    "item_embeddings = Embedding(\n",
    "    input_dim = vocab_size,\n",
    "    output_dim = EMBED_DIM,\n",
    "    embeddings_initializer = RandomUniform(minval=-0.1, maxval=0.1),\n",
    "    embeddings_regularizer=l2(REG_LAMBDA),\n",
    "    input_length=1,\n",
    "    name = 'item_embed',\n",
    "    trainable=True)\n",
    "\n",
    "# embedding of user id\n",
    "uid_input = Input(shape=(1,), dtype='int32')\n",
    "embedded_user = user_embeddings(uid_input)\n",
    "embedded_user = Reshape((EMBED_DIM,))(embedded_user)\n",
    "\n",
    "# embedding of song id\n",
    "iid_input = Input(shape=(1,), dtype='int32')\n",
    "embedded_item = item_embeddings(iid_input)\n",
    "embedded_item = Reshape((EMBED_DIM,))(embedded_item)\n",
    "\n",
    "# dot production of embedded vectors\n",
    "preds = dot([embedded_user, embedded_item], axes=1, name='dot_score')\n",
    "\n",
    "# embedding model\n",
    "user_embed_model = Model(inputs=uid_input, outputs=embedded_user)\n",
    "item_embed_model = Model(inputs=iid_input, outputs=embedded_item)\n",
    "\n",
    "model_MF = Model(inputs=[uid_input, iid_input], outputs=preds)\n",
    "model_MF.compile(\n",
    "    loss=keras.losses.mean_squared_error, \n",
    "    optimizer=RMSprop(lr=1e-3),\n",
    "#     optimizer=SGD(lr=1e-4),\n",
    "    metrics=[keras.metrics.mean_squared_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = '../model/mf/'\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "model_path = model_directory + 'mf_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_embed (Embedding)          (None, 1, 64)        1968320     input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "item_embed (Embedding)          (None, 1, 64)        23037824    input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 64)           0           user_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 64)           0           item_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dot_score (Dot)                 (None, 1)            0           reshape_13[0][0]                 \n",
      "                                                                 reshape_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 25,006,144\n",
      "Trainable params: 25,006,144\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "[2018-04-26 01:47:07] (start) train MF model\n",
      "Train on 5776897 samples, validate on 500000 samples\n",
      "Epoch 1/100\n",
      "5776897/5776897 [==============================] - 121s 21us/step - loss: 0.4795 - mean_squared_error: 0.4795 - val_loss: 0.3275 - val_mean_squared_error: 0.3275\n",
      "Epoch 2/100\n",
      "5776897/5776897 [==============================] - 108s 19us/step - loss: 0.2623 - mean_squared_error: 0.2623 - val_loss: 0.2325 - val_mean_squared_error: 0.2325\n",
      "Epoch 3/100\n",
      "5776897/5776897 [==============================] - 107s 19us/step - loss: 0.2208 - mean_squared_error: 0.2208 - val_loss: 0.2171 - val_mean_squared_error: 0.2171\n",
      "Epoch 4/100\n",
      "5776897/5776897 [==============================] - 108s 19us/step - loss: 0.2034 - mean_squared_error: 0.2034 - val_loss: 0.2101 - val_mean_squared_error: 0.2101\n",
      "Epoch 5/100\n",
      "5776897/5776897 [==============================] - 108s 19us/step - loss: 0.1909 - mean_squared_error: 0.1909 - val_loss: 0.2068 - val_mean_squared_error: 0.2068\n",
      "Epoch 6/100\n",
      "5776897/5776897 [==============================] - 107s 19us/step - loss: 0.1803 - mean_squared_error: 0.1803 - val_loss: 0.2051 - val_mean_squared_error: 0.2051\n",
      "Epoch 7/100\n",
      "5776897/5776897 [==============================] - 107s 18us/step - loss: 0.1706 - mean_squared_error: 0.1706 - val_loss: 0.2046 - val_mean_squared_error: 0.2046\n",
      "Epoch 8/100\n",
      "5776897/5776897 [==============================] - 106s 18us/step - loss: 0.1616 - mean_squared_error: 0.1616 - val_loss: 0.2050 - val_mean_squared_error: 0.2050\n",
      "Epoch 9/100\n",
      "5776897/5776897 [==============================] - 107s 18us/step - loss: 0.1533 - mean_squared_error: 0.1533 - val_loss: 0.2061 - val_mean_squared_error: 0.2061\n",
      "Epoch 10/100\n",
      "5776897/5776897 [==============================] - 107s 19us/step - loss: 0.1458 - mean_squared_error: 0.1458 - val_loss: 0.2079 - val_mean_squared_error: 0.2079\n",
      "Epoch 11/100\n",
      "5776897/5776897 [==============================] - 106s 18us/step - loss: 0.1391 - mean_squared_error: 0.1391 - val_loss: 0.2102 - val_mean_squared_error: 0.2102\n",
      "Epoch 12/100\n",
      "5776897/5776897 [==============================] - 107s 18us/step - loss: 0.1332 - mean_squared_error: 0.1332 - val_loss: 0.2126 - val_mean_squared_error: 0.2126\n",
      "[2018-04-26 02:08:49] (training finished) \n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "# early stop if no val loss improvement for 5 epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# save best model\n",
    "model_checkpoint = ModelCheckpoint(model_path, save_best_only=True, \\\n",
    "        save_weights_only=True)\n",
    "\n",
    "model_MF.summary()\n",
    "timer.start('train MF model')\n",
    "hist = model_MF.fit(\n",
    "    x=[data_train.df['user_id'], data_train.df['song_id']],\n",
    "    y=data_train.df['target'],\n",
    "    validation_data=(\n",
    "        [data_test_warm.df['user_id'], data_test_warm.df['song_id']],\n",
    "        data_test_warm.df['target']\n",
    "    ),\n",
    "    epochs=100,\n",
    "    batch_size=16*1024,\n",
    "    shuffle=True,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "model_MF.load_weights(model_path) # load the best model\n",
    "timer.output('training finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MF.load_weights(model_path) # load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 64)\n",
      "[[ 0.09333662 -0.01201833 -0.05492443  0.10049061]\n",
      " [ 0.01422359  0.01635728 -0.01554208 -0.03497285]]\n",
      "[ 0.09333661 -0.01201834 -0.05492445  0.10049061]\n"
     ]
    }
   ],
   "source": [
    "user = 1\n",
    "item_list = np.array([1,2,3,4])\n",
    "v_user = user_embed_model.predict(np.array([1, 2]))\n",
    "v_item = item_embed_model.predict(item_list)\n",
    "\n",
    "print(v_user.shape)\n",
    "\n",
    "_x = v_user @ v_item.T\n",
    "print(_x)\n",
    "\n",
    "print(model_MF.predict([\n",
    "    np.repeat(user, len(item_list)),\n",
    "    item_list\n",
    "]).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 19\n"
     ]
    }
   ],
   "source": [
    "print(len(data_test_warm.item_list[1]), len(data_test_warm.target_set[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# recall at k\n",
    "sess = tf.Session()\n",
    "v_user_all = user_embed_model.predict(np.arange(num_user))\n",
    "v_item_all = item_embed_model.predict(np.arange(num_item))\n",
    "    \n",
    "def __recall(klist, target, recommend_list):\n",
    "    den = len(target) # denominator\n",
    "    recall_value = 0.0\n",
    "    recall_list = []\n",
    "    for k in klist:\n",
    "        if den < k:\n",
    "            recall_value = 1.0\n",
    "        if recall_value == 1.0: # if it's already 1.0, it should be 1.0 after\n",
    "            recall_list.append(recall_value)\n",
    "            continue\n",
    "        recommend_set = set(recommend_list[:k])\n",
    "        num = len(target & recommend_set)\n",
    "        recall_value = float(num) / float(den)\n",
    "        recall_list.append(recall_value)\n",
    "    return recall_list\n",
    "\n",
    "\n",
    "def top_k(score_list, k):\n",
    "    ind = np.argpartition(score_list, -k)[-k:]\n",
    "    top_k_ind = list(reversed(ind[np.argsort(score_list[ind])]))\n",
    "    return top_k_ind\n",
    "\n",
    "\n",
    "def recall_mf(model, klist, data):\n",
    "    '''\n",
    "    :param klist: the list of k's in recall@k, e.g. [50, 100, 150, ...]\n",
    "    :param data: data set for evaluation\n",
    "        - user_list\n",
    "        - target_set\n",
    "        - item_set\n",
    "    :return: list(float) for recall at each k, with the same size as klist\n",
    "    '''\n",
    "    recall_at_k = []\n",
    "    max_k = max(klist)\n",
    "    t1, t2, t3, t4, t5 = 0, 0, 0, 0, 0\n",
    "    for user in data.user_list:\n",
    "        # get the corresponding embedded vectors\n",
    "        v_user = v_user_all[user]\n",
    "        v_item = v_item_all[data.item_list[user]]\n",
    "        \n",
    "        # compute the scores\n",
    "        score_list = v_user @ v_item.T\n",
    "        score_list = score_list.flatten()\n",
    "        # assert len(score_list) == len(data.item_list[user])\n",
    "        \n",
    "        k = min(max_k, len(data.item_list[user]))\n",
    "        # get the recommended list\n",
    "        indices = top_k(score_list, k)\n",
    "        recommend_list = data.item_list[user][indices]\n",
    "        \n",
    "        # evaluate recall\n",
    "        recall_at_k.append(__recall(klist, data.target_set[user], recommend_list))\n",
    "    return np.mean(recall_at_k, axis=0)\n",
    "\n",
    "\n",
    "def recall_random(klist, data):\n",
    "    recall_at_k = []\n",
    "    max_k = max(klist)\n",
    "    for i, user in enumerate(data.user_list):\n",
    "        # compute the scores\n",
    "        score_list = np.random.uniform(low=0, high=1, size=len(data.item_list[user]))\n",
    "        \n",
    "        k = min(max_k, len(data.item_list[user]))\n",
    "        indices = top_k(score_list, k)\n",
    "        recommend_list = data.item_list[user][indices]\n",
    "        \n",
    "        # evaluate recall\n",
    "        recall_at_k.append(__recall(klist, data.target_set[user], recommend_list))\n",
    "    return np.mean(recall_at_k, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will be quite slow: each user takes around 4s to run.\n",
    "\n",
    "```\n",
    "score_list = model_MF.predict([\n",
    "    np.repeat(user, len(item_list)),\n",
    "    item_list\n",
    "]).flatten()\n",
    "```\n",
    "\n",
    "**It's because fetching embeddings are slow.** So get the embedding first and use matrix multiplication!\n",
    "\n",
    "After modification, it will still take > 30 mintues (I don't know how long...)\n",
    "\n",
    "### Note!!\n",
    "\n",
    "It's **far more** better to get more from the model at one time rather than calling the model multiple times!\n",
    "\n",
    "The difference between warm and cold is due to the difference in denominator (maybe), so don't compare them with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-04-27 01:56:57] (start) evaluation\n",
      "test warm\n",
      "[ 0.20304114  0.42536847  0.60625645  0.73507289  0.81971974  0.87799102\n",
      "  0.91558068  0.94083072  0.95772601  0.96860377]\n",
      "[ 0.26750232  0.49173467  0.65854236  0.77200428  0.84624367  0.89548443\n",
      "  0.92770755  0.94943105  0.96372148  0.97313665] \n",
      "\n",
      "test cold user\n",
      "[ 0.54621682  0.67601114  0.75823514  0.81725959  0.86135843  0.89492086\n",
      "  0.92065666  0.94122077  0.95611519  0.96698845]\n",
      "[ 0.54603787  0.67667078  0.75895311  0.81788323  0.8618485   0.89507481\n",
      "  0.92072056  0.94114917  0.95592356  0.96690828] \n",
      "\n",
      "test cold item\n",
      "[ 0.50688478  0.66555159  0.7526786   0.80988863  0.84827143  0.87677757\n",
      "  0.89727115  0.91293797  0.92517445  0.93600682]\n",
      "[ 0.5093784   0.66603463  0.75262869  0.80965487  0.84824089  0.87682556\n",
      "  0.89752886  0.91310884  0.92520717  0.93597828] \n",
      "\n",
      "[2018-04-27 01:57:01] ( end ) evaluation [time elapsed: 0:00:03]\n"
     ]
    }
   ],
   "source": [
    "sub_timer.start('evaluation')\n",
    "klist = list(range(5, 51, 5))\n",
    "print('test warm')\n",
    "print(recall_random(klist, data_test_warm))\n",
    "print(recall_mf(model_MF, klist, data_test_warm), '\\n')\n",
    "\n",
    "print('test cold user')\n",
    "print(recall_random(klist, data_test_cold_user))\n",
    "print(recall_mf(model_MF, klist, data_test_cold_user), '\\n')\n",
    "\n",
    "print('test cold item')\n",
    "print(recall_random(klist, data_test_cold_item))\n",
    "print(recall_mf(model_MF, klist, data_test_cold_item), '\\n')\n",
    "sub_timer.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sub_timer.start('evaluation')\n",
    "# klist = list(range(50, 501, 50))\n",
    "# test_user_list = df_test_warm.user_id.unique()\n",
    "# print(recall_mf(model_MF, klist, test_warm_target_set, test_user_list))\n",
    "# sub_timer.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Dropout Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Unnamed: 0', 'user_id'}, set())"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_CATEGORICAL = [\n",
    "    'city', 'gender', 'registered_via', 'registration_year', \n",
    "    'registration_month', 'registration_day', 'expiration_year', \n",
    "    'expiration_month', 'expiration_day']\n",
    "user_NUMERICAL = ['age', 'weird_age', 'validate_days']\n",
    "set(df_user_context.columns) - (set(user_CATEGORICAL).union(set(user_NUMERICAL))), \\\n",
    "set(user_CATEGORICAL).intersection(set(user_NUMERICAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Unnamed: 0', 'song_id'}, set())"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_CATEGORICAL = [\n",
    "    'artist_name', 'composer', 'genre_ids', 'language', \n",
    "    'lyricist', 'song_year']\n",
    "item_NUMERICAL = [\n",
    "    'song_length', 'genre_count', 'lyricist_count',\n",
    "    'composer_count', 'artist_count', 'is_featured',\n",
    "    'artist_composer', 'artist_composer_lyricist', \n",
    "    'song_lang_boolean', 'smaller_song']\n",
    "set(df_song_context.columns) - (set(item_CATEGORICAL).union(set(item_NUMERICAL))), \\\n",
    "set(item_CATEGORICAL).intersection(set(item_NUMERICAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cold(x, recall_at):\n",
    "    embedding_prod_cold = tf.matual(x[0], x[1], transpose_b = True, name='pred_all_items')\n",
    "    _, eval_preds_cold = tf.nn.top_k(embedding_prod_cold, k=recall_at[-1], sorted=True, name='topK_net_cold')\n",
    "    return eval_preds_cold\n",
    "\n",
    "def evaluate_warm(x, recall_at):\n",
    "    embedding_prod_cold = tf.matual(x[0], x[1], transpose_b = True)\n",
    "    embedding_prod_warm = tf.sparse_add(embedding_prod_cold, x[2])\n",
    "    _, eval_preds_warm = tf.nn.top_k(embedding_prod_warm, k=recall_at[-1], sorted=True, name='topK_net_warm')\n",
    "    return eval_preds_warm\n",
    "\n",
    "def prediction(x):\n",
    "    return tf.matmul(x[0], x[1], transpose_b=True)\n",
    "\n",
    "def topk_vals(x, num_candidates):\n",
    "    tf_topk_vals, _ = tf.nn.top_k(x, k=num_candidates, sorted=True)\n",
    "    return tf.reshape(tf_topk_vals, [-1], name='select_y_vals')\n",
    "\n",
    "def topk_inds(x, num_candidates):\n",
    "    _, tf_topk_inds = tf.nn.top_k(x, k=num_candidates, sorted=True)\n",
    "    return tf.reshape(tf_topk_inds, [-1], name='select_y_vals')\n",
    "\n",
    "def random_target(x, num_candidates):\n",
    "    preds_random = tf.gather_nd(x[0], x[1])\n",
    "    return tf.reshape(preds_random, [-1], name='random_y_inds')\n",
    "\n",
    "def latent_topk_cold(x, recall_at):\n",
    "    _, tf_latent_topk_cold = tf.nn.top_k(x, k=recall_at[-1], sorted=True, name='topK_latent_cold')\n",
    "    return tf_latent_topk_cold\n",
    "\n",
    "def latent_topk_warm(x, recall_at):\n",
    "    preds_pref_latent_warm = tf.sparse_add(x[0], x[1])\n",
    "    _, tf_latent_topk_warm = tf.nn.top_k(preds_pref_latent_warm, k=recall_at[-1], sorted=True, name='topK_latent_warm')\n",
    "    return tf_latent_topk_warm\n",
    "\n",
    "def dense_batch_fc_tanh(x, units, phase, scope, do_norm=False):\n",
    "#     w_init = tf.truncated_normal_initializer(stddev=0.01)\n",
    "#     b_init = tf.zeros_initializer()\n",
    "#     h1 = Dense(units, kernel_initializer = w_init, bias_initializer = b_init)(x)\n",
    "    h1 = Dense(units, kernel_initializer = TruncatedNormal(stddev=0.01), bias_initializer = Zeros())(x)\n",
    "    if do_norm:\n",
    "        # h2 = BatchNormalization(momentum = 0.9, center=True, scale=True, training=phase)(h1)\n",
    "        h2 = BatchNormalization(momentum = 0.9, center=True, scale=True)(h1)\n",
    "        return Activation('tanh')(h2)\n",
    "    else:\n",
    "        return Activation('tanh')(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCF:\n",
    "    \"\"\"\n",
    "    main model class implementing DeepCF\n",
    "    also stores states for fast candidate generation\n",
    "\n",
    "    latent_rank_in: rank of preference model input\n",
    "    user_content_rank: rank of user content input\n",
    "    item_content_rank: rank of item content input\n",
    "    model_select: array of number of hidden unit,\n",
    "        i.e. [200,100] indicate two hidden layer with 200 units followed by 100 units\n",
    "    rank_out: rank of latent model output\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_rank_in, model_select, rank_out):\n",
    "        \n",
    "        self.rank_in = latent_rank_in\n",
    "        self.model_select = model_select\n",
    "        self.rank_out = rank_out\n",
    "\n",
    "        # inputs\n",
    "        self.Uin = None\n",
    "        self.Vin = None\n",
    "        self.Ucontent = None\n",
    "        self.Vcontent = None\n",
    "        self.phase = None\n",
    "        self.target = None\n",
    "        self.eval_trainR = None\n",
    "        self.U_pref_tf = None\n",
    "        self.V_pref_tf = None\n",
    "        self.rand_target_ui = None\n",
    "\n",
    "        # outputs in the model\n",
    "\n",
    "        self.preds = None\n",
    "        #self.updates = None\n",
    "        self.loss = None\n",
    "        self.model = None\n",
    "        self.pred_model = None\n",
    "        self.latent_eval_model = None\n",
    "\n",
    "        self.U_embedding = None\n",
    "        self.V_embedding = None\n",
    "\n",
    "        self.lr_placeholder = None\n",
    "\n",
    "        # predictor\n",
    "        self.tf_topk_vals = None\n",
    "        self.tf_topk_inds = None\n",
    "        self.preds_random = None\n",
    "        self.tf_latent_topk_cold = None\n",
    "        self.tf_latent_topk_warm = None\n",
    "        self.eval_preds_warm = None\n",
    "        self.eval_preds_cold = None\n",
    "\n",
    "    def context_model(self, tag, df_context, CATEGORICAL, NUMERICAL):\n",
    "        input_layers = []\n",
    "        embed_layers = []\n",
    "        for col in CATEGORICAL:\n",
    "            input_layer = Input(shape=(1,), name=tag + '_' + col + '_input')\n",
    "            \n",
    "            vocab_size = df_context[col].max() + 1\n",
    "            embed_size = np.power(2, int(np.ceil(np.log2(np.log2(vocab_size)))))\n",
    "            print('[%s] %-20s\\tvocab: %-8d, embed: %-4d' % (tag, col, vocab_size, embed_size))\n",
    "            embed_layer = Embedding(\n",
    "                input_dim = vocab_size,\n",
    "                output_dim = embed_size,\n",
    "                embeddings_initializer = RandomUniform(minval=-0.1, maxval=0.1),\n",
    "                embeddings_regularizer = l2(1e-4),\n",
    "                input_length = 1,\n",
    "                name = tag + '_' + col+'_embed',\n",
    "                trainable=True)\n",
    "            embed_layer = embed_layer(input_layer)\n",
    "            embed_layer = Reshape((embed_size,))(embed_layer)\n",
    "            embed_layers.append(embed_layer)\n",
    "            input_layers.append(input_layer)\n",
    "            \n",
    "        numerical_input = Input(shape=(len(NUMERICAL),), name=tag+'_numerical_input')\n",
    "        input_layers.append(numerical_input)\n",
    "        \n",
    "        preds = concatenate(embed_layers + [numerical_input], name=tag + '_content')\n",
    "#         preds = Dense(64, activation='relu', name=tag + '_content_dense1')(preds)\n",
    "#         preds = Dropout(0.5, name=tag + '_content_dropout')(preds)\n",
    "#         preds = Dense(64, name=tag + '_content_dense1')(preds)\n",
    "        return input_layers, preds\n",
    "            \n",
    "    def build_model(self):\n",
    "        self.Vin = Input(shape=(self.rank_in,), dtype='float32', name='V_in_raw')\n",
    "        self.Uin = Input(shape=(self.rank_in,), dtype='float32', name='U_in_raw')\n",
    "        \n",
    "        self.user_inputs, self.Ucontent = self.context_model(\n",
    "            'user', df_user_context, CATEGORICAL=user_CATEGORICAL, NUMERICAL=user_NUMERICAL)\n",
    "        self.item_inputs, self.Vcontent = self.context_model(\n",
    "            'item', df_song_context, CATEGORICAL=item_CATEGORICAL, NUMERICAL=item_NUMERICAL)\n",
    "        \n",
    "        #self.Vcontent = Input(shape=(self.phi_v_dim,), dtype='float32', name='V_content')\n",
    "        #self.Ucontent = Input(shape=(self.phi_u_dim,), dtype='float32', name='U_content')\n",
    "        \n",
    "        u_concat = concatenate([self.Uin, self.Ucontent])\n",
    "        v_concat = concatenate([self.Vin, self.Vcontent])\n",
    "        u_last = u_concat\n",
    "        v_last = v_concat\n",
    "        for ihid, hid in enumerate(self.model_select):\n",
    "            u_last = dense_batch_fc_tanh(u_last, hid, self.phase, 'user_layer_%d' % (ihid + 1), do_norm=True)\n",
    "            v_last = dense_batch_fc_tanh(v_last, hid, self.phase, 'item_layer_%d' % (ihid + 1), do_norm=True)\n",
    "\n",
    "        self.U_embedding = Dense(\n",
    "            self.rank_out, \n",
    "            kernel_initializer = TruncatedNormal(stddev=0.01),\n",
    "            bias_initializer = Zeros())(u_last)\n",
    "        self.V_embedding = Dense(\n",
    "            self.rank_out, \n",
    "            kernel_initializer = TruncatedNormal(stddev=0.01),\n",
    "            bias_initializer = Zeros())(v_last)\n",
    "        self.preds = dot([self.U_embedding, self.V_embedding], axes=1, name='dot_score')\n",
    "        model = Model(inputs=[self.Uin, self.Vin] + self.user_inputs + self.item_inputs, outputs=self.preds)\n",
    "        model.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
    "        self.model = model\n",
    "\n",
    "    def build_predictor(self, recall_at, num_candidates):\n",
    "        # evaluation model\n",
    "        self.eval_trainR = Input(shape=(None, ), dtype='float32', name='trainR_sparse_CPU', sparse=True) #Not sure whether a shape could have element None. If not, the shape may need an argument.\n",
    "        self.eval_preds_cold = Lambda(evaluate_cold, arguments=[recall_at])([self.U_embedding, self.V_embedding])\n",
    "        self.eval_preds_warm = Lambda(evaluate_warm, arguments=[recall_at])([self.U_embedding, self.V_embedding, self.eval_trainR])\n",
    "        model = Model(inputs=[self.U_embedding, self.V_embedding, self.eval_trainR], outputs=[self.eval_preds_cold, self.eval_preds_cold])\n",
    "        self.pred_model = model\n",
    "        # target model\n",
    "        self.U_pref_tf = Input(shape=(self.rank_in, ), dtype='float32', name='u_pref')\n",
    "        self.V_pref_tf = Input(shape=(self.rank_in, ), dtype='float32', name='v_pref')\n",
    "        self.rand_taget_ui = Input(shape=(self.rank_in, self.tank_in, ), dtype='int32', name='rand_target_ui')\n",
    "        preds_pref = Lambda(prediction)([self.U_pref_tf,self.V_pref_tf])\n",
    "        self.tf_topk_vals = Lambda(topk_vals, arguments=[num_candidates])(preds_pref)\n",
    "        self.tf_topk_inds = Lambda(topk_inds, arguments=[num_candidates])(preds_pref)\n",
    "        self.preds_random = Lambda(random_target)([preds_pref, self.rand_target_ui])\n",
    "        model = Model(inputs=[self.U_pref_tf, self.V_pref_tf, self.rand_target_ui], outputs=[self.tf_topk_vals, self.tf_topk_inds, self.preds_random])\n",
    "        self.target_model = model\n",
    "        # latent evaluation\n",
    "        self.tf_latent_topk_cold = Lambda(latent_topk_cold, arguments=[recall_at])(preds_pref)\n",
    "        self.tf_latent_topk_warm = Lambda(latent_topk_warm, arguments=[recall_at])([preds_pref, self.eval_trainR])\n",
    "        model = Model(inputs=[preds_pref, self.eval_trainR], outputs=[self.tf_latent_topk_cold, self.tf_latent_topk_warm])\n",
    "        self.latent_eval_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[user] city                \tvocab: 21      , embed: 8   \n",
      "[user] gender              \tvocab: 3       , embed: 2   \n",
      "[user] registered_via      \tvocab: 5       , embed: 4   \n",
      "[user] registration_year   \tvocab: 14      , embed: 4   \n",
      "[user] registration_month  \tvocab: 12      , embed: 4   \n",
      "[user] registration_day    \tvocab: 31      , embed: 8   \n",
      "[user] expiration_year     \tvocab: 18      , embed: 8   \n",
      "[user] expiration_month    \tvocab: 12      , embed: 4   \n",
      "[user] expiration_day      \tvocab: 31      , embed: 8   \n",
      "[item] artist_name         \tvocab: 40583   , embed: 16  \n",
      "[item] composer            \tvocab: 76064   , embed: 32  \n",
      "[item] genre_ids           \tvocab: 573     , embed: 16  \n",
      "[item] language            \tvocab: 10      , embed: 4   \n",
      "[item] lyricist            \tvocab: 33888   , embed: 16  \n",
      "[item] song_year           \tvocab: 100     , embed: 8   \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_city_input (InputLayer)    (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_gender_input (InputLayer)  (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_registered_via_input (Inpu (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_registration_year_input (I (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_registration_month_input ( (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_registration_day_input (In (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_expiration_year_input (Inp (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_expiration_month_input (In (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_expiration_day_input (Inpu (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_artist_name_input (InputLa (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_composer_input (InputLayer (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_genre_ids_input (InputLaye (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_language_input (InputLayer (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_lyricist_input (InputLayer (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_song_year_input (InputLaye (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_city_embed (Embedding)     (None, 1, 8)         168         user_city_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "user_gender_embed (Embedding)   (None, 1, 2)         6           user_gender_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "user_registered_via_embed (Embe (None, 1, 4)         20          user_registered_via_input[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "user_registration_year_embed (E (None, 1, 4)         56          user_registration_year_input[0][0\n",
      "__________________________________________________________________________________________________\n",
      "user_registration_month_embed ( (None, 1, 4)         48          user_registration_month_input[0][\n",
      "__________________________________________________________________________________________________\n",
      "user_registration_day_embed (Em (None, 1, 8)         248         user_registration_day_input[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "user_expiration_year_embed (Emb (None, 1, 8)         144         user_expiration_year_input[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "user_expiration_month_embed (Em (None, 1, 4)         48          user_expiration_month_input[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "user_expiration_day_embed (Embe (None, 1, 8)         248         user_expiration_day_input[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "item_artist_name_embed (Embeddi (None, 1, 16)        649328      item_artist_name_input[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "item_composer_embed (Embedding) (None, 1, 32)        2434048     item_composer_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "item_genre_ids_embed (Embedding (None, 1, 16)        9168        item_genre_ids_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "item_language_embed (Embedding) (None, 1, 4)         40          item_language_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "item_lyricist_embed (Embedding) (None, 1, 16)        542208      item_lyricist_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "item_song_year_embed (Embedding (None, 1, 8)         800         item_song_year_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_154 (Reshape)           (None, 8)            0           user_city_embed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_155 (Reshape)           (None, 2)            0           user_gender_embed[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "reshape_156 (Reshape)           (None, 4)            0           user_registered_via_embed[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_157 (Reshape)           (None, 4)            0           user_registration_year_embed[0][0\n",
      "__________________________________________________________________________________________________\n",
      "reshape_158 (Reshape)           (None, 4)            0           user_registration_month_embed[0][\n",
      "__________________________________________________________________________________________________\n",
      "reshape_159 (Reshape)           (None, 8)            0           user_registration_day_embed[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "reshape_160 (Reshape)           (None, 8)            0           user_expiration_year_embed[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reshape_161 (Reshape)           (None, 4)            0           user_expiration_month_embed[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "reshape_162 (Reshape)           (None, 8)            0           user_expiration_day_embed[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "user_numerical_input (InputLaye (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_163 (Reshape)           (None, 16)           0           item_artist_name_embed[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_164 (Reshape)           (None, 32)           0           item_composer_embed[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_165 (Reshape)           (None, 16)           0           item_genre_ids_embed[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_166 (Reshape)           (None, 4)            0           item_language_embed[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_167 (Reshape)           (None, 16)           0           item_lyricist_embed[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_168 (Reshape)           (None, 8)            0           item_song_year_embed[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "item_numerical_input (InputLaye (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "U_in_raw (InputLayer)           (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_content (Concatenate)      (None, 53)           0           reshape_154[0][0]                \n",
      "                                                                 reshape_155[0][0]                \n",
      "                                                                 reshape_156[0][0]                \n",
      "                                                                 reshape_157[0][0]                \n",
      "                                                                 reshape_158[0][0]                \n",
      "                                                                 reshape_159[0][0]                \n",
      "                                                                 reshape_160[0][0]                \n",
      "                                                                 reshape_161[0][0]                \n",
      "                                                                 reshape_162[0][0]                \n",
      "                                                                 user_numerical_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "V_in_raw (InputLayer)           (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_content (Concatenate)      (None, 102)          0           reshape_163[0][0]                \n",
      "                                                                 reshape_164[0][0]                \n",
      "                                                                 reshape_165[0][0]                \n",
      "                                                                 reshape_166[0][0]                \n",
      "                                                                 reshape_167[0][0]                \n",
      "                                                                 reshape_168[0][0]                \n",
      "                                                                 item_numerical_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 117)          0           U_in_raw[0][0]                   \n",
      "                                                                 user_content[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 166)          0           V_in_raw[0][0]                   \n",
      "                                                                 item_content[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_44 (Dense)                (None, 200)          23600       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_45 (Dense)                (None, 200)          33400       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 200)          800         dense_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 200)          800         dense_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 200)          0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 200)          0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_46 (Dense)                (None, 100)          20100       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_47 (Dense)                (None, 100)          20100       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 100)          400         dense_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 100)          400         dense_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 100)          0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 100)          0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 100)          10100       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 100)          10100       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_score (Dot)                 (None, 1)            0           dense_48[0][0]                   \n",
      "                                                                 dense_49[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,756,378\n",
      "Trainable params: 3,755,178\n",
      "Non-trainable params: 1,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_rank_in = 64\n",
    "model_select = [200, 100]\n",
    "rank_out = 100\n",
    "\n",
    "dropout_net = DeepCF(latent_rank_in, model_select, rank_out)\n",
    "dropout_net.build_model()\n",
    "dropout_net.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dropout_net = DeepCF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_input(X):\n",
    "    # generate feature matrix from the data frame\n",
    "    return [X[col] for col in cat] + [X.loc[:, NUMERICAL]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user content\n",
    "df_user_context\n",
    "df_song_context\n",
    "# item content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing as prep\n",
    "\n",
    "def prep_standardize(x):\n",
    "    \"\"\"\n",
    "    takes sparse input and compute standardized version\n",
    "\n",
    "    Note:\n",
    "        cap at 5 std\n",
    "\n",
    "    :param x: 2D scipy sparse data array to standardize (column-wise), must support row indexing\n",
    "    :return: the object to perform scale (stores mean/std) for inference, as well as the scaled x\n",
    "    \"\"\"\n",
    "    x_nzrow = x.any(axis=1)\n",
    "    scaler = prep.StandardScaler().fit(x[x_nzrow, :])\n",
    "    x_scaled = np.copy(x)\n",
    "    x_scaled[x_nzrow, :] = scaler.transform(x_scaled[x_nzrow, :])\n",
    "    x_scaled[x_scaled > 5] = 5\n",
    "    x_scaled[x_scaled < -5] = -5\n",
    "    x_scaled[np.absolute(x_scaled) < 1e-5] = 0\n",
    "    return scaler, x_scaled\n",
    "\n",
    "\n",
    "_, u_pref_scaled = prep_standardize(v_user_all)\n",
    "_, v_pref_scaled = prep_standardize(v_item_all)\n",
    "v_pref_expanded = np.vstack([v_pref_scaled, np.zeros_like(v_pref_scaled[0, :])])\n",
    "v_pref_last = v_pref_scaled.shape[0] # the last v_pref_scaled TODO: maybe a all zero?\n",
    "u_pref_expanded = np.vstack([u_pref_scaled, np.zeros_like(u_pref_scaled[0, :])])\n",
    "u_pref_last = u_pref_scaled.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 10\n",
    "n_scores_user = 1000\n",
    "\n",
    "row_index = np.copy(data_train.user_list)\n",
    "for epoch in range(num_epoch):\n",
    "    np.random.shuffle(row_index)\n",
    "    # generate user batches\n",
    "    for b in utils.batch(row_index, user_batch_size):\n",
    "        # n_step: number of batch\n",
    "        n_step += 1\n",
    "        # prep targets\n",
    "        # For each user, n_scores_user of the top (defined by the scores of \n",
    "        # dot product of WMF latent vectors) items are included in the batch\n",
    "        # Besides that, n_scores_user of random items with their scores\n",
    "        # (calculated as above) are also included in the batch, which means\n",
    "        # the ratio is 1:1 for topN and randomN\n",
    "        target_users = np.repeat(b, n_scores_user)\n",
    "        target_users_rand = np.repeat(np.arange(len(b)), n_scores_user)\n",
    "        target_items_rand = [np.random.choice(v_pref.shape[0], n_scores_user) for _ in b]\n",
    "        target_items_rand = np.array(target_items_rand).flatten()\n",
    "        target_ui_rand = np.transpose(np.vstack([target_users_rand, target_items_rand]))\n",
    "\n",
    "        # TODO keras: get score from WMF latent vectors\n",
    "        # [target_scores, target_items, random_scores] = sess.run(\n",
    "        #     [dropout_net.tf_topk_vals, dropout_net.tf_topk_inds, dropout_net.preds_random],\n",
    "        #     feed_dict={\n",
    "        #         dropout_net.U_pref_tf: u_pref[b, :],\n",
    "        #         dropout_net.V_pref_tf: v_pref,\n",
    "        #         dropout_net.rand_target_ui: target_ui_rand\n",
    "        #     }\n",
    "        # )\n",
    "        target_scores, target_items, random_scores = dropout_net.target_model.predict([\n",
    "            u_pref[b, :], v_pref, target_ui_rand])\n",
    "\n",
    "        # merge topN and randomN items per user\n",
    "        target_scores = np.append(target_scores, random_scores)\n",
    "        target_items = np.append(target_items, target_items_rand)\n",
    "        target_users = np.append(target_users, target_users)\n",
    "\n",
    "        n_targets = len(target_scores)\n",
    "        perm = np.random.permutation(n_targets)\n",
    "        n_targets = min(n_targets, max_data_per_step)\n",
    "        data_batch = [(n, min(n + data_batch_size, n_targets)) for n in range(0, n_targets, data_batch_size)]\n",
    "        f_batch = 0\n",
    "        for (start, stop) in data_batch:\n",
    "            batch_perm = perm[start:stop]\n",
    "            batch_users = target_users[batch_perm]\n",
    "            batch_items = target_items[batch_perm]\n",
    "            if dropout != 0:\n",
    "                # dropout * batch_size of users/items are set to pref_last\n",
    "                n_to_drop = int(np.floor(dropout * len(batch_perm)))\n",
    "                perm_user = np.random.permutation(len(batch_perm))[:n_to_drop]\n",
    "                perm_item = np.random.permutation(len(batch_perm))[:n_to_drop]\n",
    "                batch_v_pref = np.copy(batch_items)\n",
    "                batch_u_pref = np.copy(batch_users)\n",
    "                batch_v_pref[perm_user] = v_pref_last\n",
    "                batch_u_pref[perm_item] = u_pref_last\n",
    "            else:\n",
    "                batch_v_pref = batch_items\n",
    "                batch_u_pref = batch_users\n",
    "\n",
    "            # TODO keras replacement for calculate loss\n",
    "            # _, _, loss_out = sess.run(\n",
    "            #     [dropout_net.preds, dropout_net.updates, dropout_net.loss],\n",
    "            #     feed_dict={\n",
    "            #         dropout_net.Uin: u_pref_expanded[batch_u_pref, :],\n",
    "            #         dropout_net.Vin: v_pref_expanded[batch_v_pref, :],\n",
    "            #         dropout_net.Ucontent: user_content[batch_users, :].todense(),\n",
    "            #         dropout_net.Vcontent: item_content[batch_items, :].todense(),\n",
    "            #         #\n",
    "            #         dropout_net.target: target_scores[batch_perm],\n",
    "            #         dropout_net.lr_placeholder: _lr,\n",
    "            #         dropout_net.phase: 1\n",
    "            #     }\n",
    "            # )\n",
    "            loss_out = dropout_net.model.train_on_batch([\n",
    "                u_pref_expanded[batch_u_pref, :], # Uin\n",
    "                v_pref_expanded[batch_v_pref, :], # Vin\n",
    "                user_content[batch_users, :].todense(), # Ucontent\n",
    "                item_content[batch_items, :].todense(), # Vcontent\n",
    "                #\n",
    "                target_scores[batch_perm], # target\n",
    "                _lr, # lr_placeholder\n",
    "                1 # phase\n",
    "            ])\n",
    "\n",
    "\n",
    "            f_batch += loss_out\n",
    "            if np.isnan(f_batch):\n",
    "                raise Exception('f is nan')\n",
    "\n",
    "        n_batch_trained += len(data_batch)\n",
    "        # learning rate decay: exponentially\n",
    "        if n_step % _decay_lr_every == 0:\n",
    "            _lr = _lr_decay * _lr\n",
    "            print('decayed lr:' + str(_lr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
