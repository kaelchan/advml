{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-06 10:40:29\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    def __init__(self):\n",
    "        self.info = 'main'\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def start(self, info):\n",
    "        self.info = info\n",
    "        self.start_time = time.time()\n",
    "        self.checkpoint('start', elapsed_on=False)\n",
    "    \n",
    "    def end(self):\n",
    "        self.checkpoint(' end ')\n",
    "        \n",
    "    def checkpoint(self, tag, elapsed_on=True):\n",
    "        if elapsed_on:\n",
    "            elapsed = datetime.timedelta(seconds=round(time.time() - self.start_time))\n",
    "            expanded_info = self.info + ' [time elapsed: %s]' % str(elapsed)\n",
    "        else:\n",
    "            expanded_info = self.info\n",
    "        self.output(tag, info=expanded_info)\n",
    "        \n",
    "    def output(self, tag=' '*5, info=''):\n",
    "        if type(info) != type(''):\n",
    "            info = str(info)\n",
    "        print('[%s] (%s) %s' % (Timer.get_current_time(), tag, info))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_current_time():\n",
    "        return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "timer = Timer()\n",
    "sub_timer = Timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-06 10:40:29] (start) Load Data\n"
     ]
    }
   ],
   "source": [
    "timer.start('Load Data')\n",
    "# directory = '../data/split/'\n",
    "# df_train = pd.read_csv(directory + 'train.csv')\n",
    "# df_test_warm = pd.read_csv(directory + 'test_warm.csv')\n",
    "# df_test_cold_user = pd.read_csv(directory + 'test_cold_user.csv')\n",
    "# df_test_cold_item = pd.read_csv(directory + 'test_cold_item.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-06 10:40:36] (context) Load Data [time elapsed: 0:00:07]\n"
     ]
    }
   ],
   "source": [
    "directory = '../data/context/'\n",
    "df_event_context = pd.read_csv(directory + 'event_context.csv')\n",
    "df_song_context = pd.read_csv(directory + 'song_context.csv')\n",
    "df_user_context = pd.read_csv(directory + 'user_context.csv')\n",
    "df_event_context.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df_song_context.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df_user_context.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "timer.checkpoint('context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30755\n",
      "359966\n"
     ]
    }
   ],
   "source": [
    "num_user = len(df_user_context.user_id.unique())\n",
    "num_item = len(df_song_context.song_id.unique())\n",
    "print (num_user)\n",
    "print (num_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, name):\n",
    "        '''\n",
    "        user_list: list(int), the list of user id's used in the dataset\n",
    "        target_set: list(set), set of target items for each user\n",
    "        item_list: list(numpy array), list of items used in the dataset for each user\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.df = None\n",
    "        self.user_list = None\n",
    "        self.item_list = None\n",
    "        self.target_set = None\n",
    "    \n",
    "    def load(self, filename):\n",
    "        self.df = pd.read_csv(filename)\n",
    "        # prepare user list\n",
    "        self.user_list = self.df['user_id'].unique()\n",
    "        \n",
    "        # prepare item list\n",
    "        self.item_list = [[] for i in range(num_user)]\n",
    "        self.df.apply(\n",
    "            lambda row: self.item_list[row['user_id']].append(row['song_id']),\n",
    "            axis=1\n",
    "        )\n",
    "        self.item_list = list(map(np.array, self.item_list))\n",
    "        \n",
    "        # prepare target set\n",
    "        self.target_set = [set() for i in range(num_user)]\n",
    "        self.df[self.df['target'] == 1].apply(\n",
    "            lambda row: self.target_set[row['user_id']].add(row['song_id']),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "def load_split(name):\n",
    "    directory = '../data/split/'\n",
    "    data = Data(name)\n",
    "    data.load(directory + name + '.csv')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train = load_split('train')\n",
    "# data_test_warm = load_split('test_warm')\n",
    "# data_test_cold_user = load_split('test_cold_user')\n",
    "# data_test_cold_item = load_split('test_cold_item')\n",
    "# timer.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dump the class for more efficient data preparing\n",
    "# import pickle\n",
    "# with open('../data/split/data_train.pickle', 'wb') as handle:\n",
    "#     pickle.dump(data_train, handle)\n",
    "# with open('../data/split/data_test_cold_user.pickle', 'wb') as handle:\n",
    "#     pickle.dump(data_test_cold_user, handle)\n",
    "# with open('../data/split/data_test_cold_item.pickle', 'wb') as handle:\n",
    "#     pickle.dump(data_test_cold_item, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-06 10:40:40] ( end ) Load Data [time elapsed: 0:00:11]\n"
     ]
    }
   ],
   "source": [
    "# load the data class\n",
    "import pickle\n",
    "with open('../data/split/data_train.pickle', 'rb') as handle:\n",
    "    data_train = pickle.load(handle)\n",
    "with open('../data/split/data_test_warm.pickle', 'rb') as handle:\n",
    "    data_test_warm = pickle.load(handle)\n",
    "with open('../data/split/data_test_cold_user.pickle', 'rb') as handle:\n",
    "    data_test_cold_user = pickle.load(handle)\n",
    "with open('../data/split/data_test_cold_item.pickle', 'rb') as handle:\n",
    "    data_test_cold_item = pickle.load(handle)\n",
    "timer.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "data_test_warm.df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "data_test_cold_user.df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "data_test_cold_item.df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Reshape, Lambda, Multiply\n",
    "from keras.layers.merge import concatenate, dot\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import RandomUniform, RandomNormal, TruncatedNormal, Zeros\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and load the MF model for comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "REG_LAMBDA = 0\n",
    "EMBED_DIM = 64\n",
    "\n",
    "vocab_size = num_user\n",
    "user_embeddings = Embedding(\n",
    "    input_dim = vocab_size,\n",
    "    output_dim = EMBED_DIM,\n",
    "    embeddings_initializer = RandomUniform(minval=-0.1, maxval=0.1),\n",
    "    embeddings_regularizer = l2(REG_LAMBDA),\n",
    "    input_length = 1,\n",
    "    name = 'user_embed',\n",
    "    trainable=True)\n",
    "\n",
    "vocab_size = num_item\n",
    "item_embeddings = Embedding(\n",
    "    input_dim = vocab_size,\n",
    "    output_dim = EMBED_DIM,\n",
    "    embeddings_initializer = RandomUniform(minval=-0.1, maxval=0.1),\n",
    "    embeddings_regularizer=l2(REG_LAMBDA),\n",
    "    input_length=1,\n",
    "    name = 'item_embed',\n",
    "    trainable=True)\n",
    "\n",
    "# embedding of user id\n",
    "uid_input = Input(shape=(1,), dtype='int32')\n",
    "embedded_user = user_embeddings(uid_input)\n",
    "embedded_user = Reshape((EMBED_DIM,))(embedded_user)\n",
    "\n",
    "# embedding of song id\n",
    "iid_input = Input(shape=(1,), dtype='int32')\n",
    "embedded_item = item_embeddings(iid_input)\n",
    "embedded_item = Reshape((EMBED_DIM,))(embedded_item)\n",
    "\n",
    "# dot production of embedded vectors\n",
    "preds = dot([embedded_user, embedded_item], axes=1, name='dot_score')\n",
    "\n",
    "# embedding model\n",
    "user_embed_model = Model(inputs=uid_input, outputs=embedded_user)\n",
    "item_embed_model = Model(inputs=iid_input, outputs=embedded_item)\n",
    "\n",
    "model_MF = Model(inputs=[uid_input, iid_input], outputs=preds)\n",
    "model_MF.compile(\n",
    "    loss=keras.losses.mean_squared_error, \n",
    "    optimizer=RMSprop(lr=1e-3),\n",
    "#     optimizer=SGD(lr=1e-4),\n",
    "    metrics=[keras.metrics.mean_squared_error])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = '../model/mf/'\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "model_path = model_directory + 'mf_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the best model\n",
    "model_MF.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_top_k(score_list, k):\n",
    "    ind = np.argpartition(score_list, -k)[-k:]\n",
    "    top_k_ind = list(reversed(ind[np.argsort(score_list[ind])]))\n",
    "    return np.array(top_k_ind)\n",
    "\n",
    "# try to implement a two-dimensional top_k\n",
    "def two_dim_top_k(a, k):\n",
    "    return np.array([single_top_k(row, k) for row in a])\n",
    "\n",
    "def top_k(a, k):\n",
    "    if len(a.shape) == 1:\n",
    "        return single_top_k(a, k)\n",
    "    elif len(a.shape) == 2:\n",
    "        return two_dim_top_k(a, k)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# recall at k\n",
    "sess = tf.Session()\n",
    "v_user_all = user_embed_model.predict(np.arange(num_user))\n",
    "v_item_all = item_embed_model.predict(np.arange(num_item))\n",
    "    \n",
    "def __recall(klist, target, recommend_list):\n",
    "    den = len(target) # denominator\n",
    "    recall_value = 0.0\n",
    "    recall_list = []\n",
    "    for k in klist:\n",
    "        if den < k:\n",
    "            recall_value = 1.0\n",
    "        if recall_value == 1.0: # if it's already 1.0, it should be 1.0 after\n",
    "            recall_list.append(recall_value)\n",
    "            continue\n",
    "        recommend_set = set(recommend_list[:k])\n",
    "        num = len(target & recommend_set)\n",
    "        recall_value = float(num) / float(den)\n",
    "        recall_list.append(recall_value)\n",
    "    return recall_list\n",
    "\n",
    "\n",
    "def recall_mf(model, klist, data):\n",
    "    '''\n",
    "    :param klist: the list of k's in recall@k, e.g. [50, 100, 150, ...]\n",
    "    :param data: data set for evaluation\n",
    "        - user_list\n",
    "        - target_set\n",
    "        - item_set\n",
    "    :return: list(float) for recall at each k, with the same size as klist\n",
    "    '''\n",
    "    recall_at_k = []\n",
    "    max_k = max(klist)\n",
    "    t1, t2, t3, t4, t5 = 0, 0, 0, 0, 0\n",
    "    for user in data.user_list:\n",
    "        # get the corresponding embedded vectors\n",
    "        v_user = v_user_all[user]\n",
    "        v_item = v_item_all[data.item_list[user]]\n",
    "        \n",
    "        # compute the scores\n",
    "        #score_list = v_user @ v_item.T\n",
    "        score_list = np.matmul(v_user, v_item.T)\n",
    "        score_list = score_list.flatten()\n",
    "        # assert len(score_list) == len(data.item_list[user])\n",
    "        \n",
    "        k = min(max_k, len(data.item_list[user]))\n",
    "        # get the recommended list\n",
    "        indices = top_k(score_list, k)\n",
    "        recommend_list = data.item_list[user][indices]\n",
    "        \n",
    "        # evaluate recall\n",
    "        recall_at_k.append(__recall(klist, data.target_set[user], recommend_list))\n",
    "    return np.mean(recall_at_k, axis=0)\n",
    "\n",
    "\n",
    "def recall_random(klist, data):\n",
    "    recall_at_k = []\n",
    "    max_k = max(klist)\n",
    "    for i, user in enumerate(data.user_list):\n",
    "        # compute the scores\n",
    "        score_list = np.random.uniform(low=0, high=1, size=len(data.item_list[user]))\n",
    "        \n",
    "        k = min(max_k, len(data.item_list[user]))\n",
    "        indices = top_k(score_list, k)\n",
    "        recommend_list = data.item_list[user][indices]\n",
    "        \n",
    "        # evaluate recall\n",
    "        recall_at_k.append(__recall(klist, data.target_set[user], recommend_list))\n",
    "    return np.mean(recall_at_k, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stopwatch():\n",
    "    def __init__(self, info=''):\n",
    "        self.total = 0\n",
    "        self.info = info\n",
    "    \n",
    "    def clear(self):\n",
    "        self.total = 0\n",
    "    \n",
    "    def tic(self):\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def toc(self):\n",
    "        self.total += time.time() - self.start_time\n",
    "    \n",
    "    def show(self):\n",
    "        print('%.3f seconds \\t %s' % (self.total, self.info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stopwatch():\n",
    "    def __init__(self, info=''):\n",
    "        self.total = 0\n",
    "        self.info = info\n",
    "    \n",
    "    def clear(self):\n",
    "        self.total = 0\n",
    "    \n",
    "    def tic(self):\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def toc(self):\n",
    "        self.total += time.time() - self.start_time\n",
    "    \n",
    "    def show(self):\n",
    "        print('%.3f seconds \\t %s' % (self.total, self.info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'user_id'}, set())"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_CATEGORICAL = [\n",
    "    'city', 'gender', 'registered_via', 'registration_year', \n",
    "    'registration_month', 'registration_day', 'expiration_year', \n",
    "    'expiration_month', 'expiration_day']\n",
    "user_NUMERICAL = ['age', 'weird_age', 'validate_days']\n",
    "set(df_user_context.columns) - (set(user_CATEGORICAL).union(set(user_NUMERICAL))), \\\n",
    "set(user_CATEGORICAL).intersection(set(user_NUMERICAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'song_id'}, set())"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_CATEGORICAL = [\n",
    "    'artist_name', 'composer', 'genre_ids', 'language', \n",
    "    'lyricist', 'song_year']\n",
    "item_NUMERICAL = [\n",
    "    'song_length', 'genre_count', 'lyricist_count',\n",
    "    'composer_count', 'artist_count', 'is_featured',\n",
    "    'artist_composer', 'artist_composer_lyricist', \n",
    "    'song_lang_boolean', 'smaller_song']\n",
    "set(df_song_context.columns) - (set(item_CATEGORICAL).union(set(item_NUMERICAL))), \\\n",
    "set(item_CATEGORICAL).intersection(set(item_NUMERICAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_df(df):\n",
    "    ret = df.merge(df_user_context, on='user_id', how='left')\n",
    "    ret = ret.merge(df_song_context, on='song_id', how='left')\n",
    "    for col in user_CATEGORICAL + item_CATEGORICAL + ['user_id', 'song_id']:\n",
    "        ret[col] = ret[col].astype('category')\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = merge_df(data_train.df)\n",
    "test_warm = merge_df(data_test_warm.df)\n",
    "test_cold_user = merge_df(data_test_cold_user.df)\n",
    "test_cold_item = merge_df(data_test_cold_item.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate(df):\n",
    "    X = df.drop(columns=['target'])\n",
    "    y = df['target']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train.drop(columns=['target'])\n",
    "y = train['target']\n",
    "X_trn, X_val, y_trn, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "d_trn = lgb.Dataset(X_trn, y_trn)\n",
    "d_val = lgb.Dataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_train = prepare_lgb_data(train)\n",
    "X_warm, y_warm = separate(test_warm)\n",
    "X_cold_user, y_cold_user = separate(test_cold_user)\n",
    "X_cold_item, y_cold_item = separate(test_cold_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary', # objective is the goal\n",
    "#     'objective': 'mse',\n",
    "    'metric': 'auc',\n",
    "    'boosting': 'gbdt',\n",
    "    'learning_rate': 0.3,\n",
    "    'verbose': 0,\n",
    "    'num_leaves': 108,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'bagging_freq': 1,\n",
    "    'bagging_seed': 1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'feature_fraction_seed': 1,\n",
    "    'max_bin': 256,\n",
    "    'max_depth': 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'binary', 'metric': 'auc', 'boosting': 'gbdt', 'learning_rate': 0.3, 'verbose': 0, 'num_leaves': 108, 'bagging_fraction': 0.95, 'bagging_freq': 1, 'bagging_seed': 1, 'feature_fraction': 0.9, 'feature_fraction_seed': 1, 'max_bin': 256, 'max_depth': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:671: UserWarning: categorical_feature in param dict is overrided.\n",
      "  warnings.warn('categorical_feature in param dict is overrided.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\tvalid_0's auc: 0.624413\n",
      "[10]\tvalid_0's auc: 0.647502\n",
      "[15]\tvalid_0's auc: 0.669232\n",
      "[20]\tvalid_0's auc: 0.680157\n",
      "[25]\tvalid_0's auc: 0.688815\n",
      "[30]\tvalid_0's auc: 0.696188\n",
      "[35]\tvalid_0's auc: 0.701484\n",
      "[40]\tvalid_0's auc: 0.705898\n",
      "[45]\tvalid_0's auc: 0.709309\n",
      "[50]\tvalid_0's auc: 0.712761\n",
      "[55]\tvalid_0's auc: 0.715467\n",
      "[60]\tvalid_0's auc: 0.718131\n",
      "[65]\tvalid_0's auc: 0.720152\n",
      "[70]\tvalid_0's auc: 0.7223\n",
      "[75]\tvalid_0's auc: 0.724336\n",
      "[80]\tvalid_0's auc: 0.725736\n",
      "[85]\tvalid_0's auc: 0.727161\n",
      "[90]\tvalid_0's auc: 0.728666\n",
      "[95]\tvalid_0's auc: 0.730042\n",
      "[100]\tvalid_0's auc: 0.731042\n",
      "[105]\tvalid_0's auc: 0.73216\n",
      "[110]\tvalid_0's auc: 0.733189\n",
      "[115]\tvalid_0's auc: 0.733993\n",
      "[120]\tvalid_0's auc: 0.734891\n",
      "[125]\tvalid_0's auc: 0.735588\n",
      "[130]\tvalid_0's auc: 0.736574\n",
      "[135]\tvalid_0's auc: 0.737265\n",
      "[140]\tvalid_0's auc: 0.737872\n",
      "[145]\tvalid_0's auc: 0.738666\n",
      "[150]\tvalid_0's auc: 0.73929\n",
      "[155]\tvalid_0's auc: 0.73991\n",
      "[160]\tvalid_0's auc: 0.740887\n",
      "[165]\tvalid_0's auc: 0.741485\n",
      "[170]\tvalid_0's auc: 0.741949\n",
      "[175]\tvalid_0's auc: 0.742399\n",
      "[180]\tvalid_0's auc: 0.742734\n",
      "[185]\tvalid_0's auc: 0.743074\n",
      "[190]\tvalid_0's auc: 0.743457\n",
      "[195]\tvalid_0's auc: 0.744605\n",
      "[200]\tvalid_0's auc: 0.745015\n",
      "[205]\tvalid_0's auc: 0.745292\n",
      "[210]\tvalid_0's auc: 0.745648\n",
      "[215]\tvalid_0's auc: 0.74585\n",
      "[220]\tvalid_0's auc: 0.746282\n",
      "[225]\tvalid_0's auc: 0.746517\n",
      "[230]\tvalid_0's auc: 0.746949\n",
      "[235]\tvalid_0's auc: 0.747204\n",
      "[240]\tvalid_0's auc: 0.747448\n",
      "[245]\tvalid_0's auc: 0.747789\n",
      "[250]\tvalid_0's auc: 0.748188\n",
      "[255]\tvalid_0's auc: 0.748576\n",
      "[260]\tvalid_0's auc: 0.749006\n",
      "[265]\tvalid_0's auc: 0.749172\n",
      "[270]\tvalid_0's auc: 0.749378\n",
      "[275]\tvalid_0's auc: 0.74962\n",
      "[280]\tvalid_0's auc: 0.749944\n",
      "[285]\tvalid_0's auc: 0.750099\n",
      "[290]\tvalid_0's auc: 0.750581\n",
      "[295]\tvalid_0's auc: 0.750849\n",
      "[300]\tvalid_0's auc: 0.751064\n",
      "[305]\tvalid_0's auc: 0.751324\n",
      "[310]\tvalid_0's auc: 0.751411\n",
      "[315]\tvalid_0's auc: 0.751885\n",
      "[320]\tvalid_0's auc: 0.752001\n",
      "[325]\tvalid_0's auc: 0.752126\n",
      "[330]\tvalid_0's auc: 0.752376\n",
      "[335]\tvalid_0's auc: 0.752617\n",
      "[340]\tvalid_0's auc: 0.752859\n",
      "[345]\tvalid_0's auc: 0.752985\n",
      "[350]\tvalid_0's auc: 0.75313\n",
      "[355]\tvalid_0's auc: 0.753205\n",
      "[360]\tvalid_0's auc: 0.753352\n",
      "[365]\tvalid_0's auc: 0.754272\n",
      "[370]\tvalid_0's auc: 0.75451\n",
      "[375]\tvalid_0's auc: 0.754769\n",
      "[380]\tvalid_0's auc: 0.755207\n",
      "[385]\tvalid_0's auc: 0.755342\n",
      "[390]\tvalid_0's auc: 0.755507\n",
      "[395]\tvalid_0's auc: 0.755623\n",
      "[400]\tvalid_0's auc: 0.755978\n",
      "[405]\tvalid_0's auc: 0.756161\n",
      "[410]\tvalid_0's auc: 0.756328\n",
      "[415]\tvalid_0's auc: 0.756462\n",
      "[420]\tvalid_0's auc: 0.756577\n",
      "[425]\tvalid_0's auc: 0.756632\n",
      "[430]\tvalid_0's auc: 0.757062\n",
      "[435]\tvalid_0's auc: 0.757138\n",
      "[440]\tvalid_0's auc: 0.757209\n",
      "[445]\tvalid_0's auc: 0.75744\n",
      "[450]\tvalid_0's auc: 0.757558\n",
      "[455]\tvalid_0's auc: 0.757667\n",
      "[460]\tvalid_0's auc: 0.757798\n",
      "[465]\tvalid_0's auc: 0.7579\n",
      "[470]\tvalid_0's auc: 0.758028\n",
      "[475]\tvalid_0's auc: 0.758205\n",
      "[480]\tvalid_0's auc: 0.758374\n",
      "[485]\tvalid_0's auc: 0.758771\n",
      "[490]\tvalid_0's auc: 0.758871\n",
      "[495]\tvalid_0's auc: 0.758961\n",
      "[500]\tvalid_0's auc: 0.759132\n",
      "[505]\tvalid_0's auc: 0.75916\n",
      "[510]\tvalid_0's auc: 0.759246\n",
      "[515]\tvalid_0's auc: 0.759337\n",
      "[520]\tvalid_0's auc: 0.759454\n",
      "[525]\tvalid_0's auc: 0.759709\n",
      "[530]\tvalid_0's auc: 0.759779\n",
      "[535]\tvalid_0's auc: 0.759953\n",
      "[540]\tvalid_0's auc: 0.760228\n",
      "[545]\tvalid_0's auc: 0.760311\n",
      "[550]\tvalid_0's auc: 0.760688\n",
      "[555]\tvalid_0's auc: 0.760804\n",
      "[560]\tvalid_0's auc: 0.760951\n",
      "[565]\tvalid_0's auc: 0.761214\n",
      "[570]\tvalid_0's auc: 0.761298\n",
      "[575]\tvalid_0's auc: 0.761334\n",
      "[580]\tvalid_0's auc: 0.761398\n",
      "[585]\tvalid_0's auc: 0.761484\n",
      "[590]\tvalid_0's auc: 0.761482\n",
      "[595]\tvalid_0's auc: 0.761678\n",
      "[600]\tvalid_0's auc: 0.76194\n",
      "[605]\tvalid_0's auc: 0.762043\n",
      "[610]\tvalid_0's auc: 0.762144\n",
      "[615]\tvalid_0's auc: 0.762216\n",
      "[620]\tvalid_0's auc: 0.762311\n",
      "[625]\tvalid_0's auc: 0.762405\n",
      "[630]\tvalid_0's auc: 0.76248\n",
      "[635]\tvalid_0's auc: 0.762764\n",
      "[640]\tvalid_0's auc: 0.763139\n",
      "[645]\tvalid_0's auc: 0.763201\n",
      "[650]\tvalid_0's auc: 0.763267\n",
      "[655]\tvalid_0's auc: 0.763309\n",
      "[660]\tvalid_0's auc: 0.763346\n",
      "[665]\tvalid_0's auc: 0.763432\n",
      "[670]\tvalid_0's auc: 0.763532\n",
      "[675]\tvalid_0's auc: 0.763612\n",
      "[680]\tvalid_0's auc: 0.763647\n",
      "[685]\tvalid_0's auc: 0.763684\n",
      "[690]\tvalid_0's auc: 0.763729\n",
      "[695]\tvalid_0's auc: 0.76379\n",
      "[700]\tvalid_0's auc: 0.763844\n",
      "[705]\tvalid_0's auc: 0.763891\n",
      "[710]\tvalid_0's auc: 0.76392\n",
      "[715]\tvalid_0's auc: 0.763998\n",
      "[720]\tvalid_0's auc: 0.764063\n",
      "[725]\tvalid_0's auc: 0.76411\n",
      "[730]\tvalid_0's auc: 0.764168\n",
      "[735]\tvalid_0's auc: 0.764206\n",
      "[740]\tvalid_0's auc: 0.764252\n",
      "[745]\tvalid_0's auc: 0.764306\n",
      "[750]\tvalid_0's auc: 0.764318\n",
      "[755]\tvalid_0's auc: 0.764351\n",
      "[760]\tvalid_0's auc: 0.764384\n",
      "[765]\tvalid_0's auc: 0.764403\n",
      "[770]\tvalid_0's auc: 0.764424\n",
      "[775]\tvalid_0's auc: 0.764497\n",
      "[780]\tvalid_0's auc: 0.764554\n",
      "[785]\tvalid_0's auc: 0.764608\n",
      "[790]\tvalid_0's auc: 0.764685\n",
      "[795]\tvalid_0's auc: 0.764732\n",
      "[800]\tvalid_0's auc: 0.764744\n",
      "[805]\tvalid_0's auc: 0.76477\n",
      "[810]\tvalid_0's auc: 0.764792\n",
      "[815]\tvalid_0's auc: 0.76486\n",
      "[820]\tvalid_0's auc: 0.764984\n",
      "[825]\tvalid_0's auc: 0.764985\n",
      "[830]\tvalid_0's auc: 0.765012\n",
      "[835]\tvalid_0's auc: 0.765053\n",
      "[840]\tvalid_0's auc: 0.765125\n",
      "[845]\tvalid_0's auc: 0.765172\n",
      "[850]\tvalid_0's auc: 0.76521\n",
      "[855]\tvalid_0's auc: 0.765213\n",
      "[860]\tvalid_0's auc: 0.765245\n",
      "[865]\tvalid_0's auc: 0.765307\n",
      "[870]\tvalid_0's auc: 0.765325\n",
      "[875]\tvalid_0's auc: 0.765352\n",
      "[880]\tvalid_0's auc: 0.765554\n",
      "[885]\tvalid_0's auc: 0.76557\n",
      "[890]\tvalid_0's auc: 0.765611\n",
      "[895]\tvalid_0's auc: 0.765609\n",
      "[900]\tvalid_0's auc: 0.765636\n",
      "[905]\tvalid_0's auc: 0.765645\n",
      "[910]\tvalid_0's auc: 0.76576\n",
      "[915]\tvalid_0's auc: 0.765826\n",
      "[920]\tvalid_0's auc: 0.765843\n",
      "[925]\tvalid_0's auc: 0.765875\n",
      "[930]\tvalid_0's auc: 0.765913\n",
      "[935]\tvalid_0's auc: 0.765951\n",
      "[940]\tvalid_0's auc: 0.765961\n",
      "[945]\tvalid_0's auc: 0.765997\n",
      "[950]\tvalid_0's auc: 0.766054\n",
      "[955]\tvalid_0's auc: 0.766075\n",
      "[960]\tvalid_0's auc: 0.766082\n",
      "[965]\tvalid_0's auc: 0.766075\n",
      "[970]\tvalid_0's auc: 0.766065\n",
      "[975]\tvalid_0's auc: 0.766078\n",
      "[980]\tvalid_0's auc: 0.766144\n",
      "[985]\tvalid_0's auc: 0.766167\n",
      "[990]\tvalid_0's auc: 0.766192\n",
      "[995]\tvalid_0's auc: 0.766223\n",
      "[1000]\tvalid_0's auc: 0.766251\n",
      "[1005]\tvalid_0's auc: 0.76623\n",
      "[1010]\tvalid_0's auc: 0.76624\n",
      "[1015]\tvalid_0's auc: 0.766241\n",
      "[1020]\tvalid_0's auc: 0.766255\n",
      "[1025]\tvalid_0's auc: 0.766311\n",
      "[1030]\tvalid_0's auc: 0.766393\n",
      "[1035]\tvalid_0's auc: 0.766363\n",
      "[1040]\tvalid_0's auc: 0.766388\n",
      "[1045]\tvalid_0's auc: 0.766395\n",
      "[1050]\tvalid_0's auc: 0.766446\n",
      "[1055]\tvalid_0's auc: 0.766463\n",
      "[1060]\tvalid_0's auc: 0.76648\n",
      "[1065]\tvalid_0's auc: 0.766517\n",
      "[1070]\tvalid_0's auc: 0.766539\n",
      "[1075]\tvalid_0's auc: 0.76656\n",
      "[1080]\tvalid_0's auc: 0.766577\n",
      "[1085]\tvalid_0's auc: 0.766572\n",
      "[1090]\tvalid_0's auc: 0.766595\n",
      "[1095]\tvalid_0's auc: 0.766621\n",
      "[1100]\tvalid_0's auc: 0.766607\n",
      "[1105]\tvalid_0's auc: 0.766725\n",
      "[1110]\tvalid_0's auc: 0.766786\n",
      "[1115]\tvalid_0's auc: 0.766793\n",
      "[1120]\tvalid_0's auc: 0.766771\n",
      "[1125]\tvalid_0's auc: 0.76681\n",
      "[1130]\tvalid_0's auc: 0.766821\n",
      "[1135]\tvalid_0's auc: 0.766865\n",
      "[1140]\tvalid_0's auc: 0.766904\n",
      "[1145]\tvalid_0's auc: 0.766941\n",
      "[1150]\tvalid_0's auc: 0.766942\n",
      "[1155]\tvalid_0's auc: 0.766954\n",
      "[1160]\tvalid_0's auc: 0.766957\n",
      "[1165]\tvalid_0's auc: 0.766959\n",
      "[1170]\tvalid_0's auc: 0.766978\n",
      "[1175]\tvalid_0's auc: 0.766979\n",
      "[1180]\tvalid_0's auc: 0.766999\n",
      "[1185]\tvalid_0's auc: 0.767149\n",
      "[1190]\tvalid_0's auc: 0.767125\n",
      "[1195]\tvalid_0's auc: 0.767153\n",
      "[1200]\tvalid_0's auc: 0.76718\n",
      "[1205]\tvalid_0's auc: 0.767182\n",
      "[1210]\tvalid_0's auc: 0.767194\n",
      "[1215]\tvalid_0's auc: 0.767201\n",
      "[1220]\tvalid_0's auc: 0.767214\n",
      "[1225]\tvalid_0's auc: 0.767216\n",
      "[1230]\tvalid_0's auc: 0.767206\n",
      "[1235]\tvalid_0's auc: 0.767206\n",
      "[1240]\tvalid_0's auc: 0.767232\n",
      "[1245]\tvalid_0's auc: 0.767261\n",
      "[1250]\tvalid_0's auc: 0.767284\n",
      "[1255]\tvalid_0's auc: 0.767277\n",
      "[1260]\tvalid_0's auc: 0.767314\n",
      "[1265]\tvalid_0's auc: 0.76732\n",
      "[1270]\tvalid_0's auc: 0.767338\n",
      "[1275]\tvalid_0's auc: 0.76733\n",
      "[1280]\tvalid_0's auc: 0.767382\n",
      "[1285]\tvalid_0's auc: 0.767404\n",
      "[1290]\tvalid_0's auc: 0.767427\n",
      "[1295]\tvalid_0's auc: 0.767408\n",
      "[1300]\tvalid_0's auc: 0.767396\n",
      "[1305]\tvalid_0's auc: 0.767412\n",
      "[1310]\tvalid_0's auc: 0.767381\n",
      "[1315]\tvalid_0's auc: 0.767412\n",
      "[1320]\tvalid_0's auc: 0.767414\n",
      "[1325]\tvalid_0's auc: 0.767429\n",
      "[1330]\tvalid_0's auc: 0.767447\n",
      "[1335]\tvalid_0's auc: 0.767453\n",
      "[1340]\tvalid_0's auc: 0.76747\n",
      "[1345]\tvalid_0's auc: 0.767447\n",
      "[1350]\tvalid_0's auc: 0.767468\n",
      "[1355]\tvalid_0's auc: 0.767474\n",
      "[1360]\tvalid_0's auc: 0.767598\n",
      "[1365]\tvalid_0's auc: 0.767636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1370]\tvalid_0's auc: 0.767653\n",
      "[1375]\tvalid_0's auc: 0.767653\n",
      "[1380]\tvalid_0's auc: 0.767671\n",
      "[1385]\tvalid_0's auc: 0.767669\n",
      "[1390]\tvalid_0's auc: 0.767702\n",
      "[1395]\tvalid_0's auc: 0.767704\n",
      "[1400]\tvalid_0's auc: 0.767702\n",
      "[1405]\tvalid_0's auc: 0.767706\n",
      "[1410]\tvalid_0's auc: 0.767735\n",
      "[1415]\tvalid_0's auc: 0.767755\n",
      "[1420]\tvalid_0's auc: 0.767782\n",
      "[1425]\tvalid_0's auc: 0.767791\n",
      "[1430]\tvalid_0's auc: 0.767819\n",
      "[1435]\tvalid_0's auc: 0.76782\n",
      "[1440]\tvalid_0's auc: 0.767818\n",
      "[1445]\tvalid_0's auc: 0.76781\n",
      "[1450]\tvalid_0's auc: 0.767825\n",
      "[1455]\tvalid_0's auc: 0.767795\n",
      "[1460]\tvalid_0's auc: 0.767827\n",
      "[1465]\tvalid_0's auc: 0.767846\n",
      "[1470]\tvalid_0's auc: 0.767827\n",
      "[1475]\tvalid_0's auc: 0.767815\n",
      "[1480]\tvalid_0's auc: 0.767817\n",
      "[1485]\tvalid_0's auc: 0.767833\n",
      "[1490]\tvalid_0's auc: 0.767845\n",
      "[1495]\tvalid_0's auc: 0.767842\n",
      "[1500]\tvalid_0's auc: 0.767925\n",
      "[1505]\tvalid_0's auc: 0.767927\n",
      "[1510]\tvalid_0's auc: 0.767924\n",
      "[1515]\tvalid_0's auc: 0.767905\n",
      "[1520]\tvalid_0's auc: 0.767901\n",
      "[1525]\tvalid_0's auc: 0.76793\n",
      "[1530]\tvalid_0's auc: 0.767939\n",
      "[1535]\tvalid_0's auc: 0.767926\n",
      "[1540]\tvalid_0's auc: 0.767913\n",
      "[1545]\tvalid_0's auc: 0.767924\n",
      "[1550]\tvalid_0's auc: 0.767915\n",
      "[1555]\tvalid_0's auc: 0.767924\n",
      "[1560]\tvalid_0's auc: 0.767945\n",
      "[1565]\tvalid_0's auc: 0.76797\n",
      "[1570]\tvalid_0's auc: 0.767964\n",
      "[1575]\tvalid_0's auc: 0.76797\n",
      "[1580]\tvalid_0's auc: 0.767976\n",
      "[1585]\tvalid_0's auc: 0.767968\n",
      "[1590]\tvalid_0's auc: 0.767955\n",
      "[1595]\tvalid_0's auc: 0.767964\n",
      "[1600]\tvalid_0's auc: 0.767939\n",
      "[1605]\tvalid_0's auc: 0.767958\n",
      "[1610]\tvalid_0's auc: 0.767968\n",
      "[1615]\tvalid_0's auc: 0.767972\n",
      "[1620]\tvalid_0's auc: 0.767974\n",
      "[1625]\tvalid_0's auc: 0.767964\n",
      "[1630]\tvalid_0's auc: 0.768138\n",
      "[1635]\tvalid_0's auc: 0.768182\n",
      "[1640]\tvalid_0's auc: 0.768199\n",
      "[1645]\tvalid_0's auc: 0.76823\n",
      "[1650]\tvalid_0's auc: 0.768241\n",
      "[1655]\tvalid_0's auc: 0.768222\n",
      "[1660]\tvalid_0's auc: 0.768204\n",
      "[1665]\tvalid_0's auc: 0.768203\n",
      "[1670]\tvalid_0's auc: 0.768212\n",
      "[1675]\tvalid_0's auc: 0.768213\n",
      "[1680]\tvalid_0's auc: 0.768204\n",
      "[1685]\tvalid_0's auc: 0.76821\n",
      "[1690]\tvalid_0's auc: 0.768233\n",
      "[1695]\tvalid_0's auc: 0.76825\n",
      "[1700]\tvalid_0's auc: 0.768274\n",
      "[1705]\tvalid_0's auc: 0.768282\n",
      "[1710]\tvalid_0's auc: 0.768302\n",
      "[1715]\tvalid_0's auc: 0.768288\n",
      "[1720]\tvalid_0's auc: 0.7683\n",
      "[1725]\tvalid_0's auc: 0.768321\n",
      "[1730]\tvalid_0's auc: 0.768322\n",
      "[1735]\tvalid_0's auc: 0.768351\n",
      "[1740]\tvalid_0's auc: 0.768364\n",
      "[1745]\tvalid_0's auc: 0.768352\n",
      "[1750]\tvalid_0's auc: 0.768345\n",
      "[1755]\tvalid_0's auc: 0.768328\n",
      "[1760]\tvalid_0's auc: 0.768333\n",
      "[1765]\tvalid_0's auc: 0.768364\n",
      "[1770]\tvalid_0's auc: 0.76836\n",
      "[1775]\tvalid_0's auc: 0.768324\n",
      "[1780]\tvalid_0's auc: 0.768425\n",
      "[1785]\tvalid_0's auc: 0.768423\n",
      "[1790]\tvalid_0's auc: 0.768455\n",
      "[1795]\tvalid_0's auc: 0.768445\n",
      "[1800]\tvalid_0's auc: 0.768461\n",
      "[1805]\tvalid_0's auc: 0.768464\n",
      "[1810]\tvalid_0's auc: 0.768459\n",
      "[1815]\tvalid_0's auc: 0.768465\n",
      "[1820]\tvalid_0's auc: 0.768462\n",
      "[1825]\tvalid_0's auc: 0.768448\n",
      "[1830]\tvalid_0's auc: 0.768465\n",
      "[1835]\tvalid_0's auc: 0.768492\n",
      "[1840]\tvalid_0's auc: 0.768537\n",
      "[1845]\tvalid_0's auc: 0.768553\n",
      "[1850]\tvalid_0's auc: 0.76856\n",
      "[1855]\tvalid_0's auc: 0.768588\n",
      "[1860]\tvalid_0's auc: 0.768595\n",
      "[1865]\tvalid_0's auc: 0.768576\n",
      "[1870]\tvalid_0's auc: 0.768572\n",
      "[1875]\tvalid_0's auc: 0.768577\n",
      "[1880]\tvalid_0's auc: 0.768614\n",
      "[1885]\tvalid_0's auc: 0.768609\n",
      "[1890]\tvalid_0's auc: 0.768618\n",
      "[1895]\tvalid_0's auc: 0.768621\n",
      "[1900]\tvalid_0's auc: 0.768617\n",
      "[1905]\tvalid_0's auc: 0.768624\n",
      "[1910]\tvalid_0's auc: 0.768628\n",
      "[1915]\tvalid_0's auc: 0.768629\n",
      "[1920]\tvalid_0's auc: 0.768623\n",
      "[1925]\tvalid_0's auc: 0.768646\n",
      "[1930]\tvalid_0's auc: 0.768674\n",
      "[1935]\tvalid_0's auc: 0.76869\n",
      "[1940]\tvalid_0's auc: 0.768689\n",
      "[1945]\tvalid_0's auc: 0.768698\n",
      "[1950]\tvalid_0's auc: 0.768691\n",
      "[1955]\tvalid_0's auc: 0.768712\n",
      "[1960]\tvalid_0's auc: 0.768711\n",
      "[1965]\tvalid_0's auc: 0.768708\n",
      "[1970]\tvalid_0's auc: 0.7687\n",
      "[1975]\tvalid_0's auc: 0.76871\n",
      "[1980]\tvalid_0's auc: 0.768727\n",
      "[1985]\tvalid_0's auc: 0.768755\n",
      "[1990]\tvalid_0's auc: 0.768745\n",
      "[1995]\tvalid_0's auc: 0.768745\n",
      "[2000]\tvalid_0's auc: 0.768746\n"
     ]
    }
   ],
   "source": [
    "timer.start('train')\n",
    "print(params)\n",
    "model_lgb = lgb.train(params, train_set=d_trn,  valid_sets=d_val, num_boost_round=2000, verbose_eval=5)\n",
    "timer.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def recall_gbdt(klist, data, X):\n",
    "    df = data.df\n",
    "    df['score'] = model_lgb.predict(X, raw_score=True)\n",
    "    recall_at_k = []\n",
    "    max_k = max(klist)\n",
    "    for user in data.user_list:\n",
    "        # compute the scores\n",
    "        score_list = np.ravel(df[df['user_id'] == user]['score'])\n",
    "        \n",
    "        # get the recommended list\n",
    "        k = min(max_k, len(data.item_list[user]))\n",
    "        indices = top_k(score_list, k)\n",
    "        recommend_list = data.item_list[user][indices]\n",
    "        \n",
    "        # evaluate recall\n",
    "        recall_at_k.append(__recall(klist, data.target_set[user], recommend_list))\n",
    "    return np.mean(recall_at_k, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def recall_score_model(klist, data, v_user_all, v_item_all):\n",
    "    recall_at_k = []\n",
    "    max_k = max(klist)\n",
    "    for user in data.user_list:\n",
    "        # get the corresponding embedded vectors\n",
    "        v_user = v_user_all[user]\n",
    "        v_item = v_item_all[data.item_list[user]]\n",
    "        \n",
    "        # compute the scores\n",
    "        score_list = np.matmul(v_user, v_item.T)\n",
    "        score_list = score_list.flatten()\n",
    "        # assert len(score_list) == len(data.item_list[user])\n",
    "        \n",
    "        k = min(max_k, len(data.item_list[user]))\n",
    "        # get the recommended list\n",
    "        indices = top_k(score_list, k)\n",
    "        recommend_list = data.item_list[user][indices]\n",
    "        \n",
    "        # evaluate recall\n",
    "        recall_at_k.append(__recall(klist, data.target_set[user], recommend_list))\n",
    "    return np.mean(recall_at_k, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-06 10:55:00] (start) evaluation of gbdt\n",
      "{'objective': 'binary', 'metric': 'auc', 'boosting': 'gbdt', 'learning_rate': 0.3, 'verbose': 0, 'num_leaves': 108, 'bagging_fraction': 0.95, 'bagging_freq': 1, 'bagging_seed': 1, 'feature_fraction': 0.9, 'feature_fraction_seed': 1, 'max_bin': 256, 'max_depth': 10, 'categorical_column': [0, 1, 2, 4, 5, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20]}\n",
      "test warm\n",
      "[ 0.20390749  0.42532135  0.60600674  0.73548504  0.82063264  0.87865076\n",
      "  0.91608587  0.9411116   0.95774275  0.96868351]\n",
      "[ 0.26750232  0.49173467  0.65854236  0.77200428  0.84624367  0.89548443\n",
      "  0.92770755  0.94943105  0.96372148  0.97313665]\n",
      "[ 0.27099846  0.49400152  0.65974946  0.77277644  0.84668869  0.89620873\n",
      "  0.92840263  0.94979444  0.96419496  0.97358254]\n",
      "\n",
      "test cold user\n",
      "[ 0.54622637  0.67519264  0.75789506  0.81717622  0.86155252  0.89500899\n",
      "  0.92081609  0.94123241  0.95603605  0.96696675]\n",
      "[ 0.54603787  0.67667078  0.75895311  0.81788323  0.8618485   0.89507481\n",
      "  0.92072056  0.94114917  0.95592356  0.96690828]\n",
      "[ 0.56303338  0.69280547  0.77375227  0.83036771  0.87182926  0.90317812\n",
      "  0.92723523  0.94609724  0.9595283   0.96978169]\n",
      "\n",
      "test cold item\n",
      "[ 0.50684283  0.66523519  0.75262702  0.80989667  0.84822556  0.8766606\n",
      "  0.89739042  0.91299611  0.92520791  0.93602665]\n",
      "[ 0.5093784   0.66603463  0.75262869  0.80965487  0.84824089  0.87682556\n",
      "  0.89752886  0.91310884  0.92520717  0.93597828]\n",
      "[ 0.53094879  0.68539434  0.7688281   0.82367419  0.86026183  0.88647507\n",
      "  0.90567748  0.91997051  0.93123106  0.94114911]\n",
      "\n",
      "[2018-05-06 11:00:30] ( end ) evaluation of gbdt [time elapsed: 0:05:30]\n"
     ]
    }
   ],
   "source": [
    "sub_timer.start('evaluation of gbdt')\n",
    "print(params)\n",
    "klist = list(range(5, 51, 5))\n",
    "# print('train')\n",
    "# print(recall_random(klist, data_train))\n",
    "# print(recall_score_model(klist, data_train, v_user_all, v_item_all))\n",
    "# print(recall_gbdt(klist, data_train, X))\n",
    "# print()\n",
    "\n",
    "print('test warm')\n",
    "print(recall_random(klist, data_test_warm))\n",
    "print(recall_score_model(klist, data_test_warm, v_user_all, v_item_all))\n",
    "print(recall_gbdt(klist, data_test_warm, X_warm))\n",
    "print()\n",
    "\n",
    "print('test cold user')\n",
    "print(recall_random(klist, data_test_cold_user))\n",
    "print(recall_score_model(klist, data_test_cold_user, v_user_all, v_item_all))\n",
    "print(recall_gbdt(klist, data_test_cold_user, X_cold_user))\n",
    "print()\n",
    "\n",
    "print('test cold item')\n",
    "print(recall_random(klist, data_test_cold_item))\n",
    "print(recall_score_model(klist, data_test_cold_item, v_user_all, v_item_all))\n",
    "print(recall_gbdt(klist, data_test_cold_item, X_cold_item))\n",
    "print()\n",
    "sub_timer.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load previous model\n",
    "# model_path = '../model/dropout/variation1.hf5'\n",
    "# dropout_net.model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archived"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
