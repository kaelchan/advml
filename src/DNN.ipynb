{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01 03:07:33\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    def __init__(self):\n",
    "        self.info = 'main'\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def start(self, info):\n",
    "        self.info = info\n",
    "        self.start_time = time.time()\n",
    "        self.checkpoint('start', elapsed_on=False)\n",
    "    \n",
    "    def end(self):\n",
    "        self.checkpoint(' end ')\n",
    "        \n",
    "    def checkpoint(self, tag, elapsed_on=True):\n",
    "        if elapsed_on:\n",
    "            elapsed = datetime.timedelta(seconds=round(time.time() - self.start_time))\n",
    "            expanded_info = self.info + ' [time elapsed: %s]' % str(elapsed)\n",
    "        else:\n",
    "            expanded_info = self.info\n",
    "        self.output(tag, info=expanded_info)\n",
    "        \n",
    "    def output(self, tag=' '*5, info=''):\n",
    "        if type(info) != type(''):\n",
    "            info = str(info)\n",
    "        print('[%s] (%s) %s' % (Timer.get_current_time(), tag, info))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_current_time():\n",
    "        return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "timer = Timer()\n",
    "sub_timer = Timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-01 03:07:33] (start) Load Data\n"
     ]
    }
   ],
   "source": [
    "timer.start('Load Data')\n",
    "# directory = '../data/split/'\n",
    "# df_train = pd.read_csv(directory + 'train.csv')\n",
    "# df_test_warm = pd.read_csv(directory + 'test_warm.csv')\n",
    "# df_test_cold_user = pd.read_csv(directory + 'test_cold_user.csv')\n",
    "# df_test_cold_item = pd.read_csv(directory + 'test_cold_item.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-01 03:07:40] (context) Load Data [time elapsed: 0:00:07]\n"
     ]
    }
   ],
   "source": [
    "directory = '../data/context/'\n",
    "df_event_context = pd.read_csv(directory + 'event_context.csv')\n",
    "df_song_context = pd.read_csv(directory + 'song_context.csv')\n",
    "df_user_context = pd.read_csv(directory + 'user_context.csv')\n",
    "timer.checkpoint('context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30755\n",
      "359966\n"
     ]
    }
   ],
   "source": [
    "num_user = len(df_user_context.user_id.unique())\n",
    "num_item = len(df_song_context.song_id.unique())\n",
    "print (num_user)\n",
    "print (num_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load target sets\n",
    "# import pickle\n",
    "# with open('../data/split/target_set.pickle', 'rb') as handle:\n",
    "#     target_set = pickle.load(handle)\n",
    "# with open('../data/split/train_target_set.pickle', 'rb') as handle:\n",
    "#     train_target_set = pickle.load(handle)\n",
    "# with open('../data/split/test_warm_target_set.pickle', 'rb') as handle:\n",
    "#     test_warm_target_set = pickle.load(handle)\n",
    "# with open('../data/split/test_cold_user_target_set.pickle', 'rb') as handle:\n",
    "#     test_cold_user_target_set = pickle.load(handle)\n",
    "# with open('../data/split/test_cold_item_target_set.pickle', 'rb') as handle:\n",
    "#     test_cold_item_target_set = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, name):\n",
    "        '''\n",
    "        user_list: list(int), the list of user id's used in the dataset\n",
    "        target_set: list(set), set of target items for each user\n",
    "        item_list: list(numpy array), list of items used in the dataset for each user\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.df = None\n",
    "        self.user_list = None\n",
    "        self.item_list = None\n",
    "        self.target_set = None\n",
    "    \n",
    "    def load(self, filename):\n",
    "        self.df = pd.read_csv(filename)\n",
    "        # prepare user list\n",
    "        self.user_list = self.df['user_id'].unique()\n",
    "        \n",
    "        # prepare item list\n",
    "        self.item_list = [[] for i in range(num_user)]\n",
    "        self.df.apply(\n",
    "            lambda row: self.item_list[row['user_id']].append(row['song_id']),\n",
    "            axis=1\n",
    "        )\n",
    "        self.item_list = list(map(np.array, self.item_list))\n",
    "        \n",
    "        # prepare target set\n",
    "        self.target_set = [set() for i in range(num_user)]\n",
    "        self.df[self.df['target'] == 1].apply(\n",
    "            lambda row: self.target_set[row['user_id']].add(row['song_id']),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "# def load_split(name):\n",
    "#     data = Data(name)\n",
    "#     # load the user ids in the data set\n",
    "#     with open('../data/split/' + name + '_user_list.pickle', 'rb') as handle:\n",
    "#         data.user_list = pickle.load(handle)\n",
    "        \n",
    "#     # load the list(set) for items in the data set with label=1\n",
    "#     with open('../data/split/' + name + '_target_set.pickle', 'rb') as handle:\n",
    "#         data.target_set = pickle.load(handle)\n",
    "        \n",
    "#     # load the list(set) for all items in the data set\n",
    "#     with open('../data/split/' + name + '_item_set.pickle', 'rb') as handle:\n",
    "#         data.item_set = pickle.load(handle)\n",
    "        \n",
    "#     return data\n",
    "\n",
    "def load_split(name):\n",
    "    directory = '../data/split/'\n",
    "    data = Data(name)\n",
    "    data.load(directory + name + '.csv')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-01 03:11:04] ( end ) Load Data [time elapsed: 0:03:30]\n"
     ]
    }
   ],
   "source": [
    "# data_train = load_split('train')\n",
    "# data_test_warm = load_split('test_warm')\n",
    "# data_test_cold_user = load_split('test_cold_user')\n",
    "# data_test_cold_item = load_split('test_cold_item')\n",
    "\n",
    "data_train = load_split('train')\n",
    "data_test_warm = load_split('test_warm')\n",
    "data_test_cold_user = load_split('test_cold_user')\n",
    "data_test_cold_item = load_split('test_cold_item')\n",
    "timer.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Reshape, Lambda\n",
    "from keras.layers.merge import concatenate, dot\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import RandomUniform, RandomNormal, TruncatedNormal, Zeros\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "REG_LAMBDA = 0\n",
    "EMBED_DIM = 64\n",
    "\n",
    "vocab_size = num_user\n",
    "user_embeddings = Embedding(\n",
    "    input_dim = vocab_size,\n",
    "    output_dim = EMBED_DIM,\n",
    "    embeddings_initializer = RandomUniform(minval=-0.1, maxval=0.1),\n",
    "    embeddings_regularizer = l2(REG_LAMBDA),\n",
    "    input_length = 1,\n",
    "    name = 'user_embed',\n",
    "    trainable=True)\n",
    "\n",
    "vocab_size = num_item\n",
    "item_embeddings = Embedding(\n",
    "    input_dim = vocab_size,\n",
    "    output_dim = EMBED_DIM,\n",
    "    embeddings_initializer = RandomUniform(minval=-0.1, maxval=0.1),\n",
    "    embeddings_regularizer=l2(REG_LAMBDA),\n",
    "    input_length=1,\n",
    "    name = 'item_embed',\n",
    "    trainable=True)\n",
    "\n",
    "# embedding of user id\n",
    "uid_input = Input(shape=(1,), dtype='int32')\n",
    "embedded_user = user_embeddings(uid_input)\n",
    "embedded_user = Reshape((EMBED_DIM,))(embedded_user)\n",
    "\n",
    "# embedding of song id\n",
    "iid_input = Input(shape=(1,), dtype='int32')\n",
    "embedded_item = item_embeddings(iid_input)\n",
    "embedded_item = Reshape((EMBED_DIM,))(embedded_item)\n",
    "\n",
    "# dot production of embedded vectors\n",
    "preds = dot([embedded_user, embedded_item], axes=1, name='dot_score')\n",
    "\n",
    "# embedding model\n",
    "user_embed_model = Model(inputs=uid_input, outputs=embedded_user)\n",
    "item_embed_model = Model(inputs=iid_input, outputs=embedded_item)\n",
    "\n",
    "model_MF = Model(inputs=[uid_input, iid_input], outputs=preds)\n",
    "model_MF.compile(\n",
    "    loss=keras.losses.mean_squared_error, \n",
    "    optimizer=RMSprop(lr=1e-3),\n",
    "#     optimizer=SGD(lr=1e-4),\n",
    "    metrics=[keras.metrics.mean_squared_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = '../model/mf/'\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "model_path = model_directory + 'mf_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_model_path = '../model/dropout/model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the best model\n",
    "model_MF.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_top_k(score_list, k):\n",
    "    ind = np.argpartition(score_list, -k)[-k:]\n",
    "    top_k_ind = list(reversed(ind[np.argsort(score_list[ind])]))\n",
    "    return np.array(top_k_ind)\n",
    "\n",
    "# try to implement a two-dimensional top_k\n",
    "def two_dim_top_k(a, k):\n",
    "    return np.array([single_top_k(row, k) for row in a])\n",
    "\n",
    "def top_k(a, k):\n",
    "    if len(a.shape) == 1:\n",
    "        return single_top_k(a, k)\n",
    "    elif len(a.shape) == 2:\n",
    "        return two_dim_top_k(a, k)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 64)\n",
      "[[  2.59375989e-01  -3.29650007e-02   4.17865440e-02   1.46565601e-01]\n",
      " [ -6.12905342e-03   2.17780974e-02  -5.20521775e-03  -2.27784272e-04]]\n",
      "[ 0.25937599 -0.032965    0.04178654  0.1465656 ]\n"
     ]
    }
   ],
   "source": [
    "user = 1\n",
    "item_list = np.array([1,2,3,4])\n",
    "v_user = user_embed_model.predict(np.array([1, 2]))\n",
    "v_item = item_embed_model.predict(item_list)\n",
    "\n",
    "print(v_user.shape)\n",
    "\n",
    "#_x = v_user @ v_item.T #require python 3.5\n",
    "_x = np.matmul(v_user, v_item.T)\n",
    "print(_x)\n",
    "\n",
    "print(model_MF.predict([\n",
    "    np.repeat(user, len(item_list)),\n",
    "    item_list\n",
    "]).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41, 19)\n"
     ]
    }
   ],
   "source": [
    "print(len(data_test_warm.item_list[1]), len(data_test_warm.target_set[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# recall at k\n",
    "sess = tf.Session()\n",
    "v_user_all = user_embed_model.predict(np.arange(num_user))\n",
    "v_item_all = item_embed_model.predict(np.arange(num_item))\n",
    "    \n",
    "def __recall(klist, target, recommend_list):\n",
    "    den = len(target) # denominator\n",
    "    recall_value = 0.0\n",
    "    recall_list = []\n",
    "    for k in klist:\n",
    "        if den < k:\n",
    "            recall_value = 1.0\n",
    "        if recall_value == 1.0: # if it's already 1.0, it should be 1.0 after\n",
    "            recall_list.append(recall_value)\n",
    "            continue\n",
    "        recommend_set = set(recommend_list[:k])\n",
    "        num = len(target & recommend_set)\n",
    "        recall_value = float(num) / float(den)\n",
    "        recall_list.append(recall_value)\n",
    "    return recall_list\n",
    "\n",
    "\n",
    "def recall_mf(model, klist, data):\n",
    "    '''\n",
    "    :param klist: the list of k's in recall@k, e.g. [50, 100, 150, ...]\n",
    "    :param data: data set for evaluation\n",
    "        - user_list\n",
    "        - target_set\n",
    "        - item_set\n",
    "    :return: list(float) for recall at each k, with the same size as klist\n",
    "    '''\n",
    "    recall_at_k = []\n",
    "    max_k = max(klist)\n",
    "    t1, t2, t3, t4, t5 = 0, 0, 0, 0, 0\n",
    "    for user in data.user_list:\n",
    "        # get the corresponding embedded vectors\n",
    "        v_user = v_user_all[user]\n",
    "        v_item = v_item_all[data.item_list[user]]\n",
    "        \n",
    "        # compute the scores\n",
    "        #score_list = v_user @ v_item.T\n",
    "        score_list = np.matmul(v_user, v_item.T)\n",
    "        score_list = score_list.flatten()\n",
    "        # assert len(score_list) == len(data.item_list[user])\n",
    "        \n",
    "        k = min(max_k, len(data.item_list[user]))\n",
    "        # get the recommended list\n",
    "        indices = top_k(score_list, k)\n",
    "        recommend_list = data.item_list[user][indices]\n",
    "        \n",
    "        # evaluate recall\n",
    "        recall_at_k.append(__recall(klist, data.target_set[user], recommend_list))\n",
    "    return np.mean(recall_at_k, axis=0)\n",
    "\n",
    "\n",
    "def recall_random(klist, data):\n",
    "    recall_at_k = []\n",
    "    max_k = max(klist)\n",
    "    for i, user in enumerate(data.user_list):\n",
    "        # compute the scores\n",
    "        score_list = np.random.uniform(low=0, high=1, size=len(data.item_list[user]))\n",
    "        \n",
    "        k = min(max_k, len(data.item_list[user]))\n",
    "        indices = top_k(score_list, k)\n",
    "        recommend_list = data.item_list[user][indices]\n",
    "        \n",
    "        # evaluate recall\n",
    "        recall_at_k.append(__recall(klist, data.target_set[user], recommend_list))\n",
    "    return np.mean(recall_at_k, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will be quite slow: each user takes around 4s to run.\n",
    "\n",
    "```\n",
    "score_list = model_MF.predict([\n",
    "    np.repeat(user, len(item_list)),\n",
    "    item_list\n",
    "]).flatten()\n",
    "```\n",
    "\n",
    "**It's because fetching embeddings are slow.** So get the embedding first and use matrix multiplication!\n",
    "\n",
    "After modification, it will still take > 30 mintues (I don't know how long...)\n",
    "\n",
    "### Note!!\n",
    "\n",
    "It's **far more** better to get more from the model at one time rather than calling the model multiple times!\n",
    "\n",
    "The difference between warm and cold is due to the difference in denominator (maybe), so don't compare them with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Dropout Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Unnamed: 0', 'user_id'}, set())"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_CATEGORICAL = [\n",
    "    'city', 'gender', 'registered_via', 'registration_year', \n",
    "    'registration_month', 'registration_day', 'expiration_year', \n",
    "    'expiration_month', 'expiration_day']\n",
    "user_NUMERICAL = ['age', 'weird_age', 'validate_days']\n",
    "set(df_user_context.columns) - (set(user_CATEGORICAL).union(set(user_NUMERICAL))), \\\n",
    "set(user_CATEGORICAL).intersection(set(user_NUMERICAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Unnamed: 0', 'song_id'}, set())"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_CATEGORICAL = [\n",
    "    'artist_name', 'composer', 'genre_ids', 'language', \n",
    "    'lyricist', 'song_year']\n",
    "item_NUMERICAL = [\n",
    "    'song_length', 'genre_count', 'lyricist_count',\n",
    "    'composer_count', 'artist_count', 'is_featured',\n",
    "    'artist_composer', 'artist_composer_lyricist', \n",
    "    'song_lang_boolean', 'smaller_song']\n",
    "set(df_song_context.columns) - (set(item_CATEGORICAL).union(set(item_NUMERICAL))), \\\n",
    "set(item_CATEGORICAL).intersection(set(item_NUMERICAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cold(x, recall_at):\n",
    "    embedding_prod_cold = tf.matual(x[0], x[1], transpose_b = True, name='pred_all_items')\n",
    "    _, eval_preds_cold = tf.nn.top_k(embedding_prod_cold, k=recall_at[-1], sorted=True, name='topK_net_cold')\n",
    "    return eval_preds_cold\n",
    "\n",
    "def evaluate_warm(x, recall_at):\n",
    "    embedding_prod_cold = tf.matual(x[0], x[1], transpose_b = True)\n",
    "    embedding_prod_warm = tf.sparse_add(embedding_prod_cold, x[2])\n",
    "    _, eval_preds_warm = tf.nn.top_k(embedding_prod_warm, k=recall_at[-1], sorted=True, name='topK_net_warm')\n",
    "    return eval_preds_warm\n",
    "\n",
    "def prediction(x):\n",
    "    return tf.matmul(x[0], x[1], transpose_b=True)\n",
    "\n",
    "def topk_vals(x, num_candidates):\n",
    "    tf_topk_vals, _ = tf.nn.top_k(x, k=num_candidates, sorted=True)\n",
    "    return tf.reshape(tf_topk_vals, [-1], name='select_y_vals')\n",
    "\n",
    "def topk_inds(x, num_candidates):\n",
    "    _, tf_topk_inds = tf.nn.top_k(x, k=num_candidates, sorted=True)\n",
    "    return tf.reshape(tf_topk_inds, [-1], name='select_y_vals')\n",
    "\n",
    "def random_target(x, num_candidates):\n",
    "    preds_random = tf.gather_nd(x[0], x[1])\n",
    "    return tf.reshape(preds_random, [-1], name='random_y_inds')\n",
    "\n",
    "def latent_topk_cold(x, recall_at):\n",
    "    _, tf_latent_topk_cold = tf.nn.top_k(x, k=recall_at[-1], sorted=True, name='topK_latent_cold')\n",
    "    return tf_latent_topk_cold\n",
    "\n",
    "def latent_topk_warm(x, recall_at):\n",
    "    preds_pref_latent_warm = tf.sparse_add(x[0], x[1])\n",
    "    _, tf_latent_topk_warm = tf.nn.top_k(preds_pref_latent_warm, k=recall_at[-1], sorted=True, name='topK_latent_warm')\n",
    "    return tf_latent_topk_warm\n",
    "\n",
    "def dense_batch_fc_tanh(x, units, scope, do_norm=False):\n",
    "#     w_init = tf.truncated_normal_initializer(stddev=0.01)\n",
    "#     b_init = tf.zeros_initializer()\n",
    "#     h1 = Dense(units, kernel_initializer = w_init, bias_initializer = b_init)(x)\n",
    "    h1 = Dense(units, kernel_initializer = TruncatedNormal(stddev=0.01), bias_initializer = Zeros())(x)\n",
    "    if do_norm:\n",
    "        # h2 = BatchNormalization(momentum = 0.9, center=True, scale=True, training=phase)(h1)\n",
    "        h2 = BatchNormalization(momentum = 0.9, center=True, scale=True)(h1)\n",
    "        return Activation('tanh')(h2)\n",
    "    else:\n",
    "        return Activation('tanh')(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCF:\n",
    "    \"\"\"\n",
    "    main model class implementing DeepCF\n",
    "    also stores states for fast candidate generation\n",
    "\n",
    "    latent_rank_in: rank of preference model input\n",
    "    user_content_rank: rank of user content input\n",
    "    item_content_rank: rank of item content input\n",
    "    model_select: array of number of hidden unit,\n",
    "        i.e. [200,100] indicate two hidden layer with 200 units followed by 100 units\n",
    "    rank_out: rank of latent model output\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_rank_in, model_select, rank_out):\n",
    "        \n",
    "        self.rank_in = latent_rank_in\n",
    "        self.model_select = model_select\n",
    "        self.rank_out = rank_out\n",
    "\n",
    "    def context_model(self, tag, df_context, CATEGORICAL, NUMERICAL):\n",
    "        input_layers = []\n",
    "        embed_layers = []\n",
    "        for col in CATEGORICAL:\n",
    "            input_layer = Input(shape=(1,), name=tag + '_' + col + '_input')\n",
    "            input_layers.append(input_layer)\n",
    "            vocab_size = df_context[col].max() + 1\n",
    "            embed_size = np.power(2, int(np.ceil(np.log2(np.log2(vocab_size)))))\n",
    "            print('[%s] %-20s\\tvocab: %-8d, embed: %-4d' % (tag, col, vocab_size, embed_size))\n",
    "            embed_layer = Embedding(\n",
    "                input_dim = vocab_size,\n",
    "                output_dim = embed_size,\n",
    "                embeddings_initializer = RandomUniform(minval=-0.1, maxval=0.1),\n",
    "                embeddings_regularizer = l2(1e-4),\n",
    "                input_length = 1,\n",
    "                name = tag + '_' + col+'_embed',\n",
    "                trainable=True)\n",
    "            embed_layer = embed_layer(input_layer)\n",
    "            embed_layer = Reshape((embed_size,))(embed_layer)\n",
    "            embed_layers.append(embed_layer)\n",
    "            \n",
    "        numerical_input = Input(shape=(len(NUMERICAL),), name=tag+'_numerical_input')\n",
    "        input_layers.append(numerical_input)\n",
    "        \n",
    "        preds = concatenate(embed_layers + [numerical_input], name=tag + '_content')\n",
    "#         preds = Dense(64, activation='relu', name=tag + '_content_dense1')(preds)\n",
    "#         preds = Dropout(0.5, name=tag + '_content_dropout')(preds)\n",
    "#         preds = Dense(64, name=tag + '_content_dense1')(preds)\n",
    "        return input_layers, preds\n",
    "            \n",
    "    def build_model(self):\n",
    "        self.Vin = Input(shape=(self.rank_in,), dtype='float32', name='V_in_raw')\n",
    "        self.Uin = Input(shape=(self.rank_in,), dtype='float32', name='U_in_raw')\n",
    "        \n",
    "        self.user_inputs, self.Ucontent = self.context_model(\n",
    "            'user', df_user_context, CATEGORICAL=user_CATEGORICAL, NUMERICAL=user_NUMERICAL)\n",
    "        self.item_inputs, self.Vcontent = self.context_model(\n",
    "            'item', df_song_context, CATEGORICAL=item_CATEGORICAL, NUMERICAL=item_NUMERICAL)\n",
    "        \n",
    "        u_concat = concatenate([self.Uin, self.Ucontent])\n",
    "        v_concat = concatenate([self.Vin, self.Vcontent])\n",
    "        u_last = u_concat\n",
    "        v_last = v_concat\n",
    "        for ihid, hid in enumerate(self.model_select):\n",
    "            u_last = dense_batch_fc_tanh(u_last, hid, 'user_layer_%d' % (ihid + 1), do_norm=True)\n",
    "            v_last = dense_batch_fc_tanh(v_last, hid, 'item_layer_%d' % (ihid + 1), do_norm=True)\n",
    "\n",
    "        self.U_embedding = Dense(\n",
    "            self.rank_out, \n",
    "            kernel_initializer = TruncatedNormal(stddev=0.01),\n",
    "            bias_initializer = Zeros())(u_last)\n",
    "        self.V_embedding = Dense(\n",
    "            self.rank_out, \n",
    "            kernel_initializer = TruncatedNormal(stddev=0.01),\n",
    "            bias_initializer = Zeros())(v_last)\n",
    "        self.preds = dot([self.U_embedding, self.V_embedding], axes=1, name='dot_score')\n",
    "        self.input_layers = [self.Uin, self.Vin] + self.user_inputs + self.item_inputs\n",
    "        \n",
    "        self.user_model = Model(inputs=[self.Uin] + self.user_inputs, outputs=self.U_embedding)\n",
    "        self.item_model = Model(inputs=[self.Vin] + self.item_inputs, outputs=self.V_embedding)\n",
    "        \n",
    "        model = Model(inputs=self.input_layers, outputs=self.preds)\n",
    "        model.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[user] city                \tvocab: 21      , embed: 8   \n",
      "[user] gender              \tvocab: 3       , embed: 2   \n",
      "[user] registered_via      \tvocab: 5       , embed: 4   \n",
      "[user] registration_year   \tvocab: 14      , embed: 4   \n",
      "[user] registration_month  \tvocab: 12      , embed: 4   \n",
      "[user] registration_day    \tvocab: 31      , embed: 8   \n",
      "[user] expiration_year     \tvocab: 18      , embed: 8   \n",
      "[user] expiration_month    \tvocab: 12      , embed: 4   \n",
      "[user] expiration_day      \tvocab: 31      , embed: 8   \n",
      "[item] artist_name         \tvocab: 40583   , embed: 16  \n",
      "[item] composer            \tvocab: 76064   , embed: 32  \n",
      "[item] genre_ids           \tvocab: 573     , embed: 16  \n",
      "[item] language            \tvocab: 10      , embed: 4   \n",
      "[item] lyricist            \tvocab: 33888   , embed: 16  \n",
      "[item] song_year           \tvocab: 100     , embed: 8   \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_city_input (InputLayer)    (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_gender_input (InputLayer)  (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_registered_via_input (Inpu (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_registration_year_input (I (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_registration_month_input ( (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_registration_day_input (In (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_expiration_year_input (Inp (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_expiration_month_input (In (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_expiration_day_input (Inpu (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_artist_name_input (InputLa (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_composer_input (InputLayer (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_genre_ids_input (InputLaye (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_language_input (InputLayer (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_lyricist_input (InputLayer (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_song_year_input (InputLaye (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_city_embed (Embedding)     (None, 1, 8)         168         user_city_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "user_gender_embed (Embedding)   (None, 1, 2)         6           user_gender_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "user_registered_via_embed (Embe (None, 1, 4)         20          user_registered_via_input[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "user_registration_year_embed (E (None, 1, 4)         56          user_registration_year_input[0][0\n",
      "__________________________________________________________________________________________________\n",
      "user_registration_month_embed ( (None, 1, 4)         48          user_registration_month_input[0][\n",
      "__________________________________________________________________________________________________\n",
      "user_registration_day_embed (Em (None, 1, 8)         248         user_registration_day_input[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "user_expiration_year_embed (Emb (None, 1, 8)         144         user_expiration_year_input[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "user_expiration_month_embed (Em (None, 1, 4)         48          user_expiration_month_input[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "user_expiration_day_embed (Embe (None, 1, 8)         248         user_expiration_day_input[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "item_artist_name_embed (Embeddi (None, 1, 16)        649328      item_artist_name_input[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "item_composer_embed (Embedding) (None, 1, 32)        2434048     item_composer_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "item_genre_ids_embed (Embedding (None, 1, 16)        9168        item_genre_ids_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "item_language_embed (Embedding) (None, 1, 4)         40          item_language_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "item_lyricist_embed (Embedding) (None, 1, 16)        542208      item_lyricist_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "item_song_year_embed (Embedding (None, 1, 8)         800         item_song_year_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 8)            0           user_city_embed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 2)            0           user_gender_embed[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 4)            0           user_registered_via_embed[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 4)            0           user_registration_year_embed[0][0\n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 4)            0           user_registration_month_embed[0][\n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 8)            0           user_registration_day_embed[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 8)            0           user_expiration_year_embed[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 4)            0           user_expiration_month_embed[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 8)            0           user_expiration_day_embed[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "user_numerical_input (InputLaye (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 16)           0           item_artist_name_embed[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 32)           0           item_composer_embed[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_16 (Reshape)            (None, 16)           0           item_genre_ids_embed[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_17 (Reshape)            (None, 4)            0           item_language_embed[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_18 (Reshape)            (None, 16)           0           item_lyricist_embed[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_19 (Reshape)            (None, 8)            0           item_song_year_embed[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "item_numerical_input (InputLaye (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "U_in_raw (InputLayer)           (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_content (Concatenate)      (None, 53)           0           reshape_5[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "                                                                 reshape_7[0][0]                  \n",
      "                                                                 reshape_8[0][0]                  \n",
      "                                                                 reshape_9[0][0]                  \n",
      "                                                                 reshape_10[0][0]                 \n",
      "                                                                 reshape_11[0][0]                 \n",
      "                                                                 reshape_12[0][0]                 \n",
      "                                                                 reshape_13[0][0]                 \n",
      "                                                                 user_numerical_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "V_in_raw (InputLayer)           (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_content (Concatenate)      (None, 102)          0           reshape_14[0][0]                 \n",
      "                                                                 reshape_15[0][0]                 \n",
      "                                                                 reshape_16[0][0]                 \n",
      "                                                                 reshape_17[0][0]                 \n",
      "                                                                 reshape_18[0][0]                 \n",
      "                                                                 reshape_19[0][0]                 \n",
      "                                                                 item_numerical_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 117)          0           U_in_raw[0][0]                   \n",
      "                                                                 user_content[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 166)          0           V_in_raw[0][0]                   \n",
      "                                                                 item_content[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200)          23600       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 200)          33400       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 200)          800         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200)          800         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 200)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 200)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          20100       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 100)          20100       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 100)          400         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 100)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 100)          10100       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 100)          10100       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dot_score (Dot)                 (None, 1)            0           dense_5[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,756,378\n",
      "Trainable params: 3,755,178\n",
      "Non-trainable params: 1,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_rank_in = 64\n",
    "model_select = [200, 100]\n",
    "rank_out = 100\n",
    "\n",
    "dropout_net = DeepCF(latent_rank_in, model_select, rank_out)\n",
    "dropout_net.build_model()\n",
    "# dropout_net.build_predictor(klist, n_scores_user)\n",
    "dropout_net.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stopwatch():\n",
    "    def __init__(self, info=''):\n",
    "        self.total = 0\n",
    "        self.info = info\n",
    "    \n",
    "    def clear(self):\n",
    "        self.total = 0\n",
    "    \n",
    "    def tic(self):\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def toc(self):\n",
    "        self.total += time.time() - self.start_time\n",
    "    \n",
    "    def show(self):\n",
    "        print('%.3f seconds \\t %s' % (self.total, self.info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_content(user_list):\n",
    "    return [df_user_context.loc[user_list, col] for col in user_CATEGORICAL] + [df_user_context.loc[user_list, user_NUMERICAL]]\n",
    "        \n",
    "def generate_item_content(item_list):\n",
    "    return [df_song_context.loc[item_list, col] for col in item_CATEGORICAL] + [df_song_context.loc[item_list, item_NUMERICAL]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare data\n",
      "begin training\n",
      "Train on 4014720 samples, validate on 446080 samples\n",
      "Epoch 1/1\n",
      "4014720/4014720 [==============================] - 602s 150us/step - loss: 0.1673 - val_loss: 0.1639\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "u_pref = v_user_all\n",
    "v_pref = v_item_all\n",
    "_, u_pref_scaled = utils.prep_standardize(u_pref)\n",
    "_, v_pref_scaled = utils.prep_standardize(v_pref)\n",
    "v_pref_expanded = np.vstack([v_pref_scaled, np.zeros_like(v_pref_scaled[0, :])])\n",
    "v_pref_last = v_pref_scaled.shape[0] # the last v_pref_scaled TODO: maybe a all zero?\n",
    "u_pref_expanded = np.vstack([u_pref_scaled, np.zeros_like(u_pref_scaled[0, :])])\n",
    "u_pref_last = u_pref_scaled.shape[0]\n",
    "\n",
    "\n",
    "# configuration\n",
    "user_batch_size = 1000\n",
    "# n_scores_user = 2500\n",
    "n_scores_user = 100\n",
    "\n",
    "data_batch_size = 100\n",
    "max_data_per_step = 2500000\n",
    "num_epoch = 1\n",
    "_lr = 0.005\n",
    "_decay_lr_every = 50\n",
    "_lr_decay = 0.1\n",
    "dropout = 0.5\n",
    "\n",
    "# counting variables\n",
    "n_step = 0\n",
    "n_batch_trained = 0\n",
    "\n",
    "# profiling\n",
    "sw = [Stopwatch() for _ in range(20)]\n",
    "row_index = np.copy(data_train.user_list)\n",
    "\n",
    "#code testing\n",
    "\n",
    "#u_pref = u_pref[:100, :]\n",
    "#v_pref = v_pref[:100, :]\n",
    "\n",
    "\n",
    "num_users = len(row_index)\n",
    "num_items = v_pref.shape[0]\n",
    "user_content_length = 10\n",
    "item_content_length = 7\n",
    "#print(u_pref.shape)\n",
    "#print(v_pref.shape)\n",
    "#print(num_users)\n",
    "#print(\"row_index: \", row_index.shape)\n",
    "#print(num_items)\n",
    "#print (u_pref_expanded.shape)\n",
    "\n",
    "print (\"prepare data\")\n",
    "u_pref_train = np.zeros([num_users * n_scores_user * 2, u_pref.shape[1]])\n",
    "v_pref_train = np.zeros([num_users * n_scores_user * 2, v_pref.shape[1]])\n",
    "scores_train = np.zeros([num_users * n_scores_user * 2, ])\n",
    "#user_content_train = np.empty([num_users * n_scores_user * 2, user_content_length])\n",
    "#item_content_train = np.empty([num_users * n_scores_user * 2, item_content_length])\n",
    "#user_content_train = []\n",
    "#item_content_train = []\n",
    "sw[0].tic()\n",
    "np.random.shuffle(row_index)\n",
    "ind = 0\n",
    "for i in row_index:\n",
    "    #print (\"user \", i)\n",
    "    \n",
    "    score_vector = np.matmul(u_pref[i], v_pref.T)\n",
    "    #target_users = np.repeat(b, n_scores_user)\n",
    "    #print(target_users.shape)\n",
    "    target_items = top_k(score_vector, k = n_scores_user).flatten()\n",
    "    \n",
    "    # get random_N\n",
    "    \n",
    "    #target_users_rand = np.repeat(np.arange(len(b)), n_scores_user)\n",
    "    target_items_rand = np.random.choice(num_items, n_scores_user)\n",
    "    target_items_rand = np.array(target_items_rand).flatten()\n",
    "    \n",
    "\n",
    "    \n",
    "    target_scores = score_vector[target_items]\n",
    "    random_scores = score_vector[target_items_rand]\n",
    "    \n",
    "    \n",
    "    # merge topN and randomN items per user\n",
    "   \n",
    "    target_scores = np.append(target_scores, random_scores)\n",
    "    target_items = np.append(target_items, target_items_rand)\n",
    "    target_users = np.repeat(i, n_scores_user)\n",
    "    target_users = np.append(target_users, np.repeat(num_users, n_scores_user))\n",
    "    np.random.shuffle(target_users)\n",
    "    \n",
    "        \n",
    "    \n",
    "    #user_content = generate_user_content(i)\n",
    "    #item_content = generate_item_content(i)\n",
    "    \n",
    "        \n",
    "    n_targets = len(target_scores)\n",
    "    \n",
    "    u_pref_train[ind:ind+n_targets] = u_pref_expanded[target_users, :]\n",
    "    v_pref_train[ind:ind+n_targets] = v_pref_expanded[target_users, :]\n",
    "    #user_content_train.extend(user_content)\n",
    "    #item_content_train.extend(item_content)\n",
    "    scores_train[ind:ind+n_targets] = target_scores\n",
    "    ind += n_targets  \n",
    "sw[0].toc()\n",
    "\n",
    "sw[1].tic()\n",
    "#print(u_pref_train.shape)\n",
    "#print(v_pref_train.shape)\n",
    "row_index = np.repeat(row_index, 2 * n_scores_user)\n",
    "user_content_train = generate_user_content(row_index)\n",
    "item_content_train = generate_item_content(row_index)\n",
    "#print(len(user_content_train))\n",
    "#print(len(item_content_train))\n",
    "sw[1].toc()\n",
    "\n",
    "print(\"begin training\")\n",
    "sw[2].tic()\n",
    "dropout_net.model.fit(\n",
    "    x = [u_pref_train, v_pref_train] + user_content_train + item_content_train,\n",
    "    y = scores_train,\n",
    "    batch_size = 100,\n",
    "    validation_split=0.1,\n",
    "    epochs = 1,\n",
    "    verbose = 1,\n",
    "    shuffle = True)\n",
    "\n",
    "sw[2].toc()\n",
    "    \n",
    "dropout_model_path = '../model/dropout/dnn_model.h5'\n",
    "dropout_net.model.save_weights(dropout_model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "565.297 seconds \t \n",
      "1\n",
      "213.430 seconds \t \n",
      "2\n",
      "601.940 seconds \t \n",
      "3\n",
      "0.000 seconds \t \n",
      "4\n",
      "0.000 seconds \t \n",
      "5\n",
      "0.000 seconds \t \n",
      "6\n",
      "0.000 seconds \t \n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    print(i)\n",
    "    sw[i].show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
