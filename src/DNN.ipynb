{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01 18:31:36\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    def __init__(self):\n",
    "        self.info = 'main'\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def start(self, info):\n",
    "        self.info = info\n",
    "        self.start_time = time.time()\n",
    "        self.checkpoint('start', elapsed_on=False)\n",
    "    \n",
    "    def end(self):\n",
    "        self.checkpoint(' end ')\n",
    "        \n",
    "    def checkpoint(self, tag, elapsed_on=True):\n",
    "        if elapsed_on:\n",
    "            elapsed = datetime.timedelta(seconds=round(time.time() - self.start_time))\n",
    "            expanded_info = self.info + ' [time elapsed: %s]' % str(elapsed)\n",
    "        else:\n",
    "            expanded_info = self.info\n",
    "        self.output(tag, info=expanded_info)\n",
    "        \n",
    "    def output(self, tag=' '*5, info=''):\n",
    "        if type(info) != type(''):\n",
    "            info = str(info)\n",
    "        print('[%s] (%s) %s' % (Timer.get_current_time(), tag, info))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_current_time():\n",
    "        return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "timer = Timer()\n",
    "sub_timer = Timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-01 18:31:36] (start) Load Data\n"
     ]
    }
   ],
   "source": [
    "timer.start('Load Data')\n",
    "# directory = '../data/split/'\n",
    "# df_train = pd.read_csv(directory + 'train.csv')\n",
    "# df_test_warm = pd.read_csv(directory + 'test_warm.csv')\n",
    "# df_test_cold_user = pd.read_csv(directory + 'test_cold_user.csv')\n",
    "# df_test_cold_item = pd.read_csv(directory + 'test_cold_item.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-01 18:31:43] (context) Load Data [time elapsed: 0:00:07]\n"
     ]
    }
   ],
   "source": [
    "directory = '../data/context/'\n",
    "df_event_context = pd.read_csv(directory + 'event_context.csv')\n",
    "df_song_context = pd.read_csv(directory + 'song_context.csv')\n",
    "df_user_context = pd.read_csv(directory + 'user_context.csv')\n",
    "timer.checkpoint('context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30755\n",
      "359966\n"
     ]
    }
   ],
   "source": [
    "num_user = len(df_user_context.user_id.unique())\n",
    "num_item = len(df_song_context.song_id.unique())\n",
    "print (num_user)\n",
    "print (num_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load target sets\n",
    "# import pickle\n",
    "# with open('../data/split/target_set.pickle', 'rb') as handle:\n",
    "#     target_set = pickle.load(handle)\n",
    "# with open('../data/split/train_target_set.pickle', 'rb') as handle:\n",
    "#     train_target_set = pickle.load(handle)\n",
    "# with open('../data/split/test_warm_target_set.pickle', 'rb') as handle:\n",
    "#     test_warm_target_set = pickle.load(handle)\n",
    "# with open('../data/split/test_cold_user_target_set.pickle', 'rb') as handle:\n",
    "#     test_cold_user_target_set = pickle.load(handle)\n",
    "# with open('../data/split/test_cold_item_target_set.pickle', 'rb') as handle:\n",
    "#     test_cold_item_target_set = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, name):\n",
    "        '''\n",
    "        user_list: list(int), the list of user id's used in the dataset\n",
    "        target_set: list(set), set of target items for each user\n",
    "        item_list: list(numpy array), list of items used in the dataset for each user\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.df = None\n",
    "        self.user_list = None\n",
    "        self.item_list = None\n",
    "        self.target_set = None\n",
    "    \n",
    "    def load(self, filename):\n",
    "        self.df = pd.read_csv(filename)\n",
    "        # prepare user list\n",
    "        self.user_list = self.df['user_id'].unique()\n",
    "        \n",
    "        # prepare item list\n",
    "        self.item_list = [[] for i in range(num_user)]\n",
    "        self.df.apply(\n",
    "            lambda row: self.item_list[row['user_id']].append(row['song_id']),\n",
    "            axis=1\n",
    "        )\n",
    "        self.item_list = list(map(np.array, self.item_list))\n",
    "        \n",
    "        # prepare target set\n",
    "        self.target_set = [set() for i in range(num_user)]\n",
    "        self.df[self.df['target'] == 1].apply(\n",
    "            lambda row: self.target_set[row['user_id']].add(row['song_id']),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "# def load_split(name):\n",
    "#     data = Data(name)\n",
    "#     # load the user ids in the data set\n",
    "#     with open('../data/split/' + name + '_user_list.pickle', 'rb') as handle:\n",
    "#         data.user_list = pickle.load(handle)\n",
    "        \n",
    "#     # load the list(set) for items in the data set with label=1\n",
    "#     with open('../data/split/' + name + '_target_set.pickle', 'rb') as handle:\n",
    "#         data.target_set = pickle.load(handle)\n",
    "        \n",
    "#     # load the list(set) for all items in the data set\n",
    "#     with open('../data/split/' + name + '_item_set.pickle', 'rb') as handle:\n",
    "#         data.item_set = pickle.load(handle)\n",
    "        \n",
    "#     return data\n",
    "\n",
    "def load_split(name):\n",
    "    directory = '../data/split/'\n",
    "    data = Data(name)\n",
    "    data.load(directory + name + '.csv')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-01 18:35:05] ( end ) Load Data [time elapsed: 0:03:29]\n"
     ]
    }
   ],
   "source": [
    "# data_train = load_split('train')\n",
    "# data_test_warm = load_split('test_warm')\n",
    "# data_test_cold_user = load_split('test_cold_user')\n",
    "# data_test_cold_item = load_split('test_cold_item')\n",
    "\n",
    "data_train = load_split('train')\n",
    "data_test_warm = load_split('test_warm')\n",
    "data_test_cold_user = load_split('test_cold_user')\n",
    "data_test_cold_item = load_split('test_cold_item')\n",
    "timer.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Reshape, Lambda\n",
    "from keras.layers.merge import concatenate, dot\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import RandomUniform, RandomNormal, TruncatedNormal, Zeros\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "REG_LAMBDA = 0\n",
    "EMBED_DIM = 64\n",
    "\n",
    "vocab_size = num_user\n",
    "user_embeddings = Embedding(\n",
    "    input_dim = vocab_size,\n",
    "    output_dim = EMBED_DIM,\n",
    "    embeddings_initializer = RandomUniform(minval=-0.1, maxval=0.1),\n",
    "    embeddings_regularizer = l2(REG_LAMBDA),\n",
    "    input_length = 1,\n",
    "    name = 'user_embed',\n",
    "    trainable=True)\n",
    "\n",
    "vocab_size = num_item\n",
    "item_embeddings = Embedding(\n",
    "    input_dim = vocab_size,\n",
    "    output_dim = EMBED_DIM,\n",
    "    embeddings_initializer = RandomUniform(minval=-0.1, maxval=0.1),\n",
    "    embeddings_regularizer=l2(REG_LAMBDA),\n",
    "    input_length=1,\n",
    "    name = 'item_embed',\n",
    "    trainable=True)\n",
    "\n",
    "# embedding of user id\n",
    "uid_input = Input(shape=(1,), dtype='int32')\n",
    "embedded_user = user_embeddings(uid_input)\n",
    "embedded_user = Reshape((EMBED_DIM,))(embedded_user)\n",
    "\n",
    "# embedding of song id\n",
    "iid_input = Input(shape=(1,), dtype='int32')\n",
    "embedded_item = item_embeddings(iid_input)\n",
    "embedded_item = Reshape((EMBED_DIM,))(embedded_item)\n",
    "\n",
    "# dot production of embedded vectors\n",
    "preds = dot([embedded_user, embedded_item], axes=1, name='dot_score')\n",
    "\n",
    "# embedding model\n",
    "user_embed_model = Model(inputs=uid_input, outputs=embedded_user)\n",
    "item_embed_model = Model(inputs=iid_input, outputs=embedded_item)\n",
    "\n",
    "model_MF = Model(inputs=[uid_input, iid_input], outputs=preds)\n",
    "model_MF.compile(\n",
    "    loss=keras.losses.mean_squared_error, \n",
    "    optimizer=RMSprop(lr=1e-3),\n",
    "#     optimizer=SGD(lr=1e-4),\n",
    "    metrics=[keras.metrics.mean_squared_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = '../model/mf/'\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "model_path = model_directory + 'mf_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_model_path = '../model/dropout/model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the best model\n",
    "model_MF.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_top_k(score_list, k):\n",
    "    ind = np.argpartition(score_list, -k)[-k:]\n",
    "    top_k_ind = list(reversed(ind[np.argsort(score_list[ind])]))\n",
    "    return np.array(top_k_ind)\n",
    "\n",
    "# try to implement a two-dimensional top_k\n",
    "def two_dim_top_k(a, k):\n",
    "    return np.array([single_top_k(row, k) for row in a])\n",
    "\n",
    "def top_k(a, k):\n",
    "    if len(a.shape) == 1:\n",
    "        return single_top_k(a, k)\n",
    "    elif len(a.shape) == 2:\n",
    "        return two_dim_top_k(a, k)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 64)\n",
      "[[  2.59375989e-01  -3.29650007e-02   4.17865440e-02   1.46565601e-01]\n",
      " [ -6.12905342e-03   2.17780974e-02  -5.20521775e-03  -2.27784272e-04]]\n",
      "[ 0.25937599 -0.032965    0.04178654  0.1465656 ]\n"
     ]
    }
   ],
   "source": [
    "user = 1\n",
    "item_list = np.array([1,2,3,4])\n",
    "v_user = user_embed_model.predict(np.array([1, 2]))\n",
    "v_item = item_embed_model.predict(item_list)\n",
    "\n",
    "print(v_user.shape)\n",
    "\n",
    "#_x = v_user @ v_item.T #require python 3.5\n",
    "_x = np.matmul(v_user, v_item.T)\n",
    "print(_x)\n",
    "\n",
    "print(model_MF.predict([\n",
    "    np.repeat(user, len(item_list)),\n",
    "    item_list\n",
    "]).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41, 19)\n"
     ]
    }
   ],
   "source": [
    "print(len(data_test_warm.item_list[1]), len(data_test_warm.target_set[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# recall at k\n",
    "sess = tf.Session()\n",
    "v_user_all = user_embed_model.predict(np.arange(num_user))\n",
    "v_item_all = item_embed_model.predict(np.arange(num_item))\n",
    "    \n",
    "def __recall(klist, target, recommend_list):\n",
    "    den = len(target) # denominator\n",
    "    recall_value = 0.0\n",
    "    recall_list = []\n",
    "    for k in klist:\n",
    "        if den < k:\n",
    "            recall_value = 1.0\n",
    "        if recall_value == 1.0: # if it's already 1.0, it should be 1.0 after\n",
    "            recall_list.append(recall_value)\n",
    "            continue\n",
    "        recommend_set = set(recommend_list[:k])\n",
    "        num = len(target & recommend_set)\n",
    "        recall_value = float(num) / float(den)\n",
    "        recall_list.append(recall_value)\n",
    "    return recall_list\n",
    "\n",
    "\n",
    "def recall_mf(model, klist, data):\n",
    "    '''\n",
    "    :param klist: the list of k's in recall@k, e.g. [50, 100, 150, ...]\n",
    "    :param data: data set for evaluation\n",
    "        - user_list\n",
    "        - target_set\n",
    "        - item_set\n",
    "    :return: list(float) for recall at each k, with the same size as klist\n",
    "    '''\n",
    "    recall_at_k = []\n",
    "    max_k = max(klist)\n",
    "    t1, t2, t3, t4, t5 = 0, 0, 0, 0, 0\n",
    "    for user in data.user_list:\n",
    "        # get the corresponding embedded vectors\n",
    "        v_user = v_user_all[user]\n",
    "        v_item = v_item_all[data.item_list[user]]\n",
    "        \n",
    "        # compute the scores\n",
    "        #score_list = v_user @ v_item.T\n",
    "        score_list = np.matmul(v_user, v_item.T)\n",
    "        score_list = score_list.flatten()\n",
    "        # assert len(score_list) == len(data.item_list[user])\n",
    "        \n",
    "        k = min(max_k, len(data.item_list[user]))\n",
    "        # get the recommended list\n",
    "        indices = top_k(score_list, k)\n",
    "        recommend_list = data.item_list[user][indices]\n",
    "        \n",
    "        # evaluate recall\n",
    "        recall_at_k.append(__recall(klist, data.target_set[user], recommend_list))\n",
    "    return np.mean(recall_at_k, axis=0)\n",
    "\n",
    "\n",
    "def recall_random(klist, data):\n",
    "    recall_at_k = []\n",
    "    max_k = max(klist)\n",
    "    for i, user in enumerate(data.user_list):\n",
    "        # compute the scores\n",
    "        score_list = np.random.uniform(low=0, high=1, size=len(data.item_list[user]))\n",
    "        \n",
    "        k = min(max_k, len(data.item_list[user]))\n",
    "        indices = top_k(score_list, k)\n",
    "        recommend_list = data.item_list[user][indices]\n",
    "        \n",
    "        # evaluate recall\n",
    "        recall_at_k.append(__recall(klist, data.target_set[user], recommend_list))\n",
    "    return np.mean(recall_at_k, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will be quite slow: each user takes around 4s to run.\n",
    "\n",
    "```\n",
    "score_list = model_MF.predict([\n",
    "    np.repeat(user, len(item_list)),\n",
    "    item_list\n",
    "]).flatten()\n",
    "```\n",
    "\n",
    "**It's because fetching embeddings are slow.** So get the embedding first and use matrix multiplication!\n",
    "\n",
    "After modification, it will still take > 30 mintues (I don't know how long...)\n",
    "\n",
    "### Note!!\n",
    "\n",
    "It's **far more** better to get more from the model at one time rather than calling the model multiple times!\n",
    "\n",
    "The difference between warm and cold is due to the difference in denominator (maybe), so don't compare them with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Dropout Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Unnamed: 0', 'user_id'}, set())"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_CATEGORICAL = [\n",
    "    'city', 'gender', 'registered_via', 'registration_year', \n",
    "    'registration_month', 'registration_day', 'expiration_year', \n",
    "    'expiration_month', 'expiration_day']\n",
    "user_NUMERICAL = ['age', 'weird_age', 'validate_days']\n",
    "set(df_user_context.columns) - (set(user_CATEGORICAL).union(set(user_NUMERICAL))), \\\n",
    "set(user_CATEGORICAL).intersection(set(user_NUMERICAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Unnamed: 0', 'song_id'}, set())"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_CATEGORICAL = [\n",
    "    'artist_name', 'composer', 'genre_ids', 'language', \n",
    "    'lyricist', 'song_year']\n",
    "item_NUMERICAL = [\n",
    "    'song_length', 'genre_count', 'lyricist_count',\n",
    "    'composer_count', 'artist_count', 'is_featured',\n",
    "    'artist_composer', 'artist_composer_lyricist', \n",
    "    'song_lang_boolean', 'smaller_song']\n",
    "set(df_song_context.columns) - (set(item_CATEGORICAL).union(set(item_NUMERICAL))), \\\n",
    "set(item_CATEGORICAL).intersection(set(item_NUMERICAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cold(x, recall_at):\n",
    "    embedding_prod_cold = tf.matual(x[0], x[1], transpose_b = True, name='pred_all_items')\n",
    "    _, eval_preds_cold = tf.nn.top_k(embedding_prod_cold, k=recall_at[-1], sorted=True, name='topK_net_cold')\n",
    "    return eval_preds_cold\n",
    "\n",
    "def evaluate_warm(x, recall_at):\n",
    "    embedding_prod_cold = tf.matual(x[0], x[1], transpose_b = True)\n",
    "    embedding_prod_warm = tf.sparse_add(embedding_prod_cold, x[2])\n",
    "    _, eval_preds_warm = tf.nn.top_k(embedding_prod_warm, k=recall_at[-1], sorted=True, name='topK_net_warm')\n",
    "    return eval_preds_warm\n",
    "\n",
    "def prediction(x):\n",
    "    return tf.matmul(x[0], x[1], transpose_b=True)\n",
    "\n",
    "def topk_vals(x, num_candidates):\n",
    "    tf_topk_vals, _ = tf.nn.top_k(x, k=num_candidates, sorted=True)\n",
    "    return tf.reshape(tf_topk_vals, [-1], name='select_y_vals')\n",
    "\n",
    "def topk_inds(x, num_candidates):\n",
    "    _, tf_topk_inds = tf.nn.top_k(x, k=num_candidates, sorted=True)\n",
    "    return tf.reshape(tf_topk_inds, [-1], name='select_y_vals')\n",
    "\n",
    "def random_target(x, num_candidates):\n",
    "    preds_random = tf.gather_nd(x[0], x[1])\n",
    "    return tf.reshape(preds_random, [-1], name='random_y_inds')\n",
    "\n",
    "def latent_topk_cold(x, recall_at):\n",
    "    _, tf_latent_topk_cold = tf.nn.top_k(x, k=recall_at[-1], sorted=True, name='topK_latent_cold')\n",
    "    return tf_latent_topk_cold\n",
    "\n",
    "def latent_topk_warm(x, recall_at):\n",
    "    preds_pref_latent_warm = tf.sparse_add(x[0], x[1])\n",
    "    _, tf_latent_topk_warm = tf.nn.top_k(preds_pref_latent_warm, k=recall_at[-1], sorted=True, name='topK_latent_warm')\n",
    "    return tf_latent_topk_warm\n",
    "\n",
    "def dense_batch_fc_tanh(x, units, scope, do_norm=False):\n",
    "#     w_init = tf.truncated_normal_initializer(stddev=0.01)\n",
    "#     b_init = tf.zeros_initializer()\n",
    "#     h1 = Dense(units, kernel_initializer = w_init, bias_initializer = b_init)(x)\n",
    "    h1 = Dense(units, kernel_initializer = TruncatedNormal(stddev=0.01), bias_initializer = Zeros())(x)\n",
    "    if do_norm:\n",
    "        # h2 = BatchNormalization(momentum = 0.9, center=True, scale=True, training=phase)(h1)\n",
    "        h2 = BatchNormalization(momentum = 0.9, center=True, scale=True)(h1)\n",
    "        return Activation('tanh')(h2)\n",
    "    else:\n",
    "        return Activation('tanh')(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCF:\n",
    "    \"\"\"\n",
    "    main model class implementing DeepCF\n",
    "    also stores states for fast candidate generation\n",
    "\n",
    "    latent_rank_in: rank of preference model input\n",
    "    user_content_rank: rank of user content input\n",
    "    item_content_rank: rank of item content input\n",
    "    model_select: array of number of hidden unit,\n",
    "        i.e. [200,100] indicate two hidden layer with 200 units followed by 100 units\n",
    "    rank_out: rank of latent model output\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_rank_in, model_select, rank_out):\n",
    "        self.rank_in = latent_rank_in\n",
    "        self.model_select = model_select\n",
    "        self.rank_out = rank_out\n",
    "\n",
    "    def context_model(self, tag, df_context, CATEGORICAL, NUMERICAL):\n",
    "        input_layers = []\n",
    "        embed_layers = []\n",
    "        for col in CATEGORICAL:\n",
    "            input_layer = Input(shape=(1,), name=tag + '_' + col + '_input')\n",
    "            input_layers.append(input_layer)\n",
    "            vocab_size = df_context[col].max() + 1\n",
    "            embed_size = np.power(2, int(np.ceil(np.log2(np.log2(vocab_size)))))\n",
    "            print('[%s] %-20s\\tvocab: %-8d, embed: %-4d' % (tag, col, vocab_size, embed_size))\n",
    "            embed_layer = Embedding(\n",
    "                input_dim = vocab_size,\n",
    "                output_dim = embed_size,\n",
    "                embeddings_initializer = RandomUniform(minval=-0.1, maxval=0.1),\n",
    "                embeddings_regularizer = l2(1e-4),\n",
    "                input_length = 1,\n",
    "                name = tag + '_' + col+'_embed',\n",
    "                trainable=True)\n",
    "            embed_layer = embed_layer(input_layer)\n",
    "            embed_layer = Reshape((embed_size,))(embed_layer)\n",
    "            embed_layers.append(embed_layer)\n",
    "            \n",
    "        numerical_input = Input(shape=(len(NUMERICAL),), name=tag+'_numerical_input')\n",
    "        input_layers.append(numerical_input)\n",
    "        \n",
    "        preds = concatenate(embed_layers + [numerical_input], name=tag + '_content')\n",
    "#         preds = Dense(64, activation='relu', name=tag + '_content_dense1')(preds)\n",
    "#         preds = Dropout(0.5, name=tag + '_content_dropout')(preds)\n",
    "#         preds = Dense(64, name=tag + '_content_dense1')(preds)\n",
    "        return input_layers, preds\n",
    "            \n",
    "    def build_model(self):\n",
    "        self.Vin = Input(shape=(self.rank_in,), dtype='float32', name='V_in_raw')\n",
    "        self.Uin = Input(shape=(self.rank_in,), dtype='float32', name='U_in_raw')\n",
    "        \n",
    "        self.user_inputs, self.Ucontent = self.context_model(\n",
    "            'user', df_user_context, CATEGORICAL=user_CATEGORICAL, NUMERICAL=user_NUMERICAL)\n",
    "        self.item_inputs, self.Vcontent = self.context_model(\n",
    "            'item', df_song_context, CATEGORICAL=item_CATEGORICAL, NUMERICAL=item_NUMERICAL)\n",
    "        \n",
    "        u_concat = concatenate([self.Uin, self.Ucontent])\n",
    "        v_concat = concatenate([self.Vin, self.Vcontent])\n",
    "        u_last = u_concat\n",
    "        v_last = v_concat\n",
    "        for ihid, hid in enumerate(self.model_select):\n",
    "            u_last = dense_batch_fc_tanh(u_last, hid, 'user_layer_%d' % (ihid + 1), do_norm=True)\n",
    "            v_last = dense_batch_fc_tanh(v_last, hid, 'item_layer_%d' % (ihid + 1), do_norm=True)\n",
    "\n",
    "        self.U_embedding = Dense(\n",
    "            self.rank_out, \n",
    "            kernel_initializer = TruncatedNormal(stddev=0.01),\n",
    "            bias_initializer = Zeros())(u_last)\n",
    "        self.V_embedding = Dense(\n",
    "            self.rank_out, \n",
    "            kernel_initializer = TruncatedNormal(stddev=0.01),\n",
    "            bias_initializer = Zeros())(v_last)\n",
    "        self.preds = dot([self.U_embedding, self.V_embedding], axes=1, name='dot_score')\n",
    "        self.input_layers = [self.Uin, self.Vin] + self.user_inputs + self.item_inputs\n",
    "        \n",
    "        self.user_model = Model(inputs=[self.Uin] + self.user_inputs, outputs=self.U_embedding)\n",
    "        self.item_model = Model(inputs=[self.Vin] + self.item_inputs, outputs=self.V_embedding)\n",
    "        \n",
    "        model = Model(inputs=self.input_layers, outputs=self.preds)\n",
    "        # optimizer = RMSprop()\n",
    "        optimizer = RMSprop(lr=1e-2)\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[user] city                \tvocab: 21      , embed: 8   \n",
      "[user] gender              \tvocab: 3       , embed: 2   \n",
      "[user] registered_via      \tvocab: 5       , embed: 4   \n",
      "[user] registration_year   \tvocab: 14      , embed: 4   \n",
      "[user] registration_month  \tvocab: 12      , embed: 4   \n",
      "[user] registration_day    \tvocab: 31      , embed: 8   \n",
      "[user] expiration_year     \tvocab: 18      , embed: 8   \n",
      "[user] expiration_month    \tvocab: 12      , embed: 4   \n",
      "[user] expiration_day      \tvocab: 31      , embed: 8   \n",
      "[item] artist_name         \tvocab: 40583   , embed: 16  \n",
      "[item] composer            \tvocab: 76064   , embed: 32  \n",
      "[item] genre_ids           \tvocab: 573     , embed: 16  \n",
      "[item] language            \tvocab: 10      , embed: 4   \n",
      "[item] lyricist            \tvocab: 33888   , embed: 16  \n",
      "[item] song_year           \tvocab: 100     , embed: 8   \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_city_input (InputLayer)    (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_gender_input (InputLayer)  (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_registered_via_input (Inpu (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_registration_year_input (I (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_registration_month_input ( (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_registration_day_input (In (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_expiration_year_input (Inp (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_expiration_month_input (In (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_expiration_day_input (Inpu (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_artist_name_input (InputLa (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_composer_input (InputLayer (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_genre_ids_input (InputLaye (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_language_input (InputLayer (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_lyricist_input (InputLayer (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_song_year_input (InputLaye (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_city_embed (Embedding)     (None, 1, 8)         168         user_city_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "user_gender_embed (Embedding)   (None, 1, 2)         6           user_gender_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "user_registered_via_embed (Embe (None, 1, 4)         20          user_registered_via_input[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "user_registration_year_embed (E (None, 1, 4)         56          user_registration_year_input[0][0\n",
      "__________________________________________________________________________________________________\n",
      "user_registration_month_embed ( (None, 1, 4)         48          user_registration_month_input[0][\n",
      "__________________________________________________________________________________________________\n",
      "user_registration_day_embed (Em (None, 1, 8)         248         user_registration_day_input[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "user_expiration_year_embed (Emb (None, 1, 8)         144         user_expiration_year_input[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "user_expiration_month_embed (Em (None, 1, 4)         48          user_expiration_month_input[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "user_expiration_day_embed (Embe (None, 1, 8)         248         user_expiration_day_input[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "item_artist_name_embed (Embeddi (None, 1, 16)        649328      item_artist_name_input[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "item_composer_embed (Embedding) (None, 1, 32)        2434048     item_composer_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "item_genre_ids_embed (Embedding (None, 1, 16)        9168        item_genre_ids_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "item_language_embed (Embedding) (None, 1, 4)         40          item_language_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "item_lyricist_embed (Embedding) (None, 1, 16)        542208      item_lyricist_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "item_song_year_embed (Embedding (None, 1, 8)         800         item_song_year_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_18 (Reshape)            (None, 8)            0           user_city_embed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_19 (Reshape)            (None, 2)            0           user_gender_embed[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "reshape_20 (Reshape)            (None, 4)            0           user_registered_via_embed[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_21 (Reshape)            (None, 4)            0           user_registration_year_embed[0][0\n",
      "__________________________________________________________________________________________________\n",
      "reshape_22 (Reshape)            (None, 4)            0           user_registration_month_embed[0][\n",
      "__________________________________________________________________________________________________\n",
      "reshape_23 (Reshape)            (None, 8)            0           user_registration_day_embed[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "reshape_24 (Reshape)            (None, 8)            0           user_expiration_year_embed[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reshape_25 (Reshape)            (None, 4)            0           user_expiration_month_embed[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "reshape_26 (Reshape)            (None, 8)            0           user_expiration_day_embed[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "user_numerical_input (InputLaye (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_27 (Reshape)            (None, 16)           0           item_artist_name_embed[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_28 (Reshape)            (None, 32)           0           item_composer_embed[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_29 (Reshape)            (None, 16)           0           item_genre_ids_embed[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_30 (Reshape)            (None, 4)            0           item_language_embed[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_31 (Reshape)            (None, 16)           0           item_lyricist_embed[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_32 (Reshape)            (None, 8)            0           item_song_year_embed[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "item_numerical_input (InputLaye (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "U_in_raw (InputLayer)           (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_content (Concatenate)      (None, 53)           0           reshape_18[0][0]                 \n",
      "                                                                 reshape_19[0][0]                 \n",
      "                                                                 reshape_20[0][0]                 \n",
      "                                                                 reshape_21[0][0]                 \n",
      "                                                                 reshape_22[0][0]                 \n",
      "                                                                 reshape_23[0][0]                 \n",
      "                                                                 reshape_24[0][0]                 \n",
      "                                                                 reshape_25[0][0]                 \n",
      "                                                                 reshape_26[0][0]                 \n",
      "                                                                 user_numerical_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "V_in_raw (InputLayer)           (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_content (Concatenate)      (None, 102)          0           reshape_27[0][0]                 \n",
      "                                                                 reshape_28[0][0]                 \n",
      "                                                                 reshape_29[0][0]                 \n",
      "                                                                 reshape_30[0][0]                 \n",
      "                                                                 reshape_31[0][0]                 \n",
      "                                                                 reshape_32[0][0]                 \n",
      "                                                                 item_numerical_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 117)          0           U_in_raw[0][0]                   \n",
      "                                                                 user_content[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 166)          0           V_in_raw[0][0]                   \n",
      "                                                                 item_content[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 200)          23600       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 200)          33400       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 200)          800         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 200)          800         dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 200)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 200)          0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 100)          20100       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 100)          20100       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 100)          400         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 100)          400         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 100)          0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 100)          0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 100)          10100       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 100)          10100       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dot_score (Dot)                 (None, 1)            0           dense_11[0][0]                   \n",
      "                                                                 dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,756,378\n",
      "Trainable params: 3,755,178\n",
      "Non-trainable params: 1,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_rank_in = 64\n",
    "model_select = [200, 100]\n",
    "rank_out = 100\n",
    "\n",
    "dropout_net = DeepCF(latent_rank_in, model_select, rank_out)\n",
    "dropout_net.build_model()\n",
    "dropout_net.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stopwatch():\n",
    "    def __init__(self, info=''):\n",
    "        self.total = 0\n",
    "        self.info = info\n",
    "    \n",
    "    def clear(self):\n",
    "        self.total = 0\n",
    "    \n",
    "    def tic(self):\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def toc(self):\n",
    "        self.total += time.time() - self.start_time\n",
    "    \n",
    "    def show(self):\n",
    "        print('%.3f seconds \\t %s' % (self.total, self.info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_content(user_list):\n",
    "    return [df_user_context.loc[user_list, col] for col in user_CATEGORICAL] + [df_user_context.loc[user_list, user_NUMERICAL]]\n",
    "        \n",
    "def generate_item_content(item_list):\n",
    "    return [df_song_context.loc[item_list, col] for col in item_CATEGORICAL] + [df_song_context.loc[item_list, item_NUMERICAL]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_array_flat_join(list_of_list):\n",
    "    ret = []\n",
    "    for l in list_of_list:\n",
    "        for e in l:\n",
    "            ret.append(e)\n",
    "    return np.array(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "user_batch_size = 1000\n",
    "# n_scores_user = 2500\n",
    "n_scores_user = 100\n",
    "\n",
    "data_batch_size = 100\n",
    "max_data_per_step = 2500000\n",
    "dropout = 0.5\n",
    "\n",
    "# counting variables\n",
    "n_step = 0\n",
    "n_batch_trained = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "u_pref = v_user_all\n",
    "v_pref = v_item_all\n",
    "_, u_pref_scaled = utils.prep_standardize(u_pref)\n",
    "_, v_pref_scaled = utils.prep_standardize(v_pref)\n",
    "v_pref_expanded = np.vstack([v_pref_scaled, np.zeros_like(v_pref_scaled[0, :])])\n",
    "v_pref_last = v_pref_scaled.shape[0] # the last v_pref_scaled TODO: maybe a all zero?\n",
    "u_pref_expanded = np.vstack([u_pref_scaled, np.zeros_like(u_pref_scaled[0, :])])\n",
    "u_pref_last = u_pref_scaled.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650.950 seconds \t \n"
     ]
    }
   ],
   "source": [
    "# get top k matrix, the i-th row is the top k items for the i-th user\n",
    "stopwatch = Stopwatch()\n",
    "stopwatch.tic()\n",
    "topK_matrix = [top_k(np.matmul(u_pref[i], v_pref.T), k = n_scores_user) for i in range(num_user)]\n",
    "topK_matrix = np.array(topK_matrix)\n",
    "stopwatch.toc()\n",
    "stopwatch.show() # around 600s = 10min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-01 22:32:43] (start) training\n",
      "(1000, 100000)\n",
      "(1000, 100000)\n",
      "(1000,)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-821b8230ae01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0msw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "timer.start('training')\n",
    "# profiling\n",
    "sw = [Stopwatch() for _ in range(20)]\n",
    "\n",
    "num_epoch = 10\n",
    "row_index = np.copy(data_train.user_list)\n",
    "for epoch in range(num_epoch):\n",
    "    sw[0].tic()\n",
    "    np.random.shuffle(row_index)\n",
    "    sw[0].toc()\n",
    "    # generate user batches\n",
    "    whole_target_scores_list = []\n",
    "    whole_target_items_list = []\n",
    "    whole_target_users_list = []\n",
    "    for b in utils.batch(row_index, user_batch_size):\n",
    "        # prep targets\n",
    "        # For each user, n_scores_user of the top (defined by the scores of \n",
    "        # dot product of WMF latent vectors) items are included in the batch\n",
    "        # Besides that, n_scores_user of random items with their scores\n",
    "        # (calculated as above) are also included in the batch, which means\n",
    "        # the ratio is 1:1 for topN and randomN\n",
    "        \n",
    "        # get top_N items for the users\n",
    "        # NOTE: if this takes long time, we can preprocess this part of data and store them\n",
    "        sw[1].tic()\n",
    "        target_users = np.repeat(b, n_scores_user)\n",
    "        # target_items = top_k(score_matrix, k = n_scores_user).flatten()\n",
    "        target_items = topK_matrix[b].flatten()\n",
    "        score_matrix = np.matmul(u_pref[b, :], v_pref.T)\n",
    "        sw[1].toc()\n",
    "        \n",
    "        # get random_N\n",
    "        sw[2].tic()\n",
    "        target_users_rand = np.repeat(b, n_scores_user)\n",
    "        target_items_rand = [np.random.choice(num_item, n_scores_user) for _ in b]\n",
    "        target_items_rand = np.array(target_items_rand).flatten()\n",
    "        sw[2].toc()\n",
    "        \n",
    "        sw[3].tic()\n",
    "        target_scores = score_matrix[:, target_items]\n",
    "        random_scores = score_matrix[:, target_items_rand]\n",
    "        sw[3].toc()\n",
    "\n",
    "        # merge topN and randomN items per user\n",
    "        sw[4].tic()\n",
    "        target_users = np.append(target_users, target_users)\n",
    "        target_items = np.append(target_items, target_items_rand)\n",
    "        target_scores = np.append(target_scores, random_scores)\n",
    "        # append to the epoch list\n",
    "        whole_target_users_list.append(target_users)\n",
    "        whole_target_items_list.append(target_items)\n",
    "        whole_target_scores_list.append(target_scores)\n",
    "        sw[4].toc()\n",
    "    \n",
    "    # joining the user batches into a epoch data\n",
    "    sw[5].tic()\n",
    "    target_users = numpy_array_flat_join(whole_target_users_list)\n",
    "    target_items = numpy_array_flat_join(whole_target_items_list)\n",
    "    target_scores = numpy_array_flat_join(whole_target_scores_list)\n",
    "    sw[5].toc()\n",
    "\n",
    "    # generate content (should before dropout)\n",
    "    sw[6].tic()\n",
    "    user_content = generate_user_content(target_users)\n",
    "    item_content = generate_item_content(target_items)\n",
    "    sw[6].toc()\n",
    "    \n",
    "    # dropout process\n",
    "    sw[7].tic()\n",
    "    n_targets = len(target_scores)\n",
    "    # n_targets = min(n_targets, max_data_per_step)\n",
    "    n_to_drop = int(np.floor(dropout * n_targets))\n",
    "    user_to_drop = np.random.choice(np.arange(n_targets), size=n_to_drop, replace=False)\n",
    "    item_to_drop = np.random.choice(np.arange(n_targets), size=n_to_drop, replace=False)\n",
    "    target_users[user_to_drop] = u_pref_last\n",
    "    target_items[item_to_drop] = v_pref_last\n",
    "    sw[7].toc()\n",
    "            \n",
    "    sw[8].tic()\n",
    "    loss_out = dropout_net.model.fit(\n",
    "        x = [u_pref_expanded[target_users, :], v_pref_expanded[target_items, :]] + user_content + item_content,\n",
    "        y = target_scores, # target\n",
    "        epochs = 1,\n",
    "        batch_size = data_batch_size,\n",
    "        shuffle = True\n",
    "    )\n",
    "    sw[8].toc()\n",
    "    dropout_model_path = '../model/dropout/exp2/model_{}.h5'.format(epoch)\n",
    "    dropout_net.model.save_weights(dropout_model_path)\n",
    "timer.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    print i,\n",
    "    sw[i].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 130us/step - loss: 0.0624\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 130us/step - loss: 0.0593\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 130us/step - loss: 0.0643\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 130us/step - loss: 0.0624\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 130us/step - loss: 0.0596\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 129us/step - loss: 0.0619\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 131us/step - loss: 0.0604\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 132us/step - loss: 0.0599\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 131us/step - loss: 0.0607\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 134us/step - loss: 0.0630\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 133us/step - loss: 0.0587\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 131us/step - loss: 0.0609\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 25s 127us/step - loss: 0.0602\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 122us/step - loss: 0.0598\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 131us/step - loss: 0.0591\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 131us/step - loss: 0.0615\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 130us/step - loss: 0.0620\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 131us/step - loss: 0.0599\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 122us/step - loss: 0.0595\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 121us/step - loss: 0.0599\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 122us/step - loss: 0.0585\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 121us/step - loss: 0.0589\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 121us/step - loss: 0.0608\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 121us/step - loss: 0.0596\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0603\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0602\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0603\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 121us/step - loss: 0.0590\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 25s 123us/step - loss: 0.0594\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 139us/step - loss: 0.0590\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 144us/step - loss: 0.0588\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 143us/step - loss: 0.0588\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 142us/step - loss: 0.0588\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 136us/step - loss: 0.0600\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 130us/step - loss: 0.0584\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 130us/step - loss: 0.0600\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 25s 127us/step - loss: 0.0591\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0584\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 121us/step - loss: 0.0598\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 25s 125us/step - loss: 0.0583\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 121us/step - loss: 0.0567\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0576\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 130us/step - loss: 0.0581\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 130us/step - loss: 0.0589\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 130us/step - loss: 0.0588\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 129us/step - loss: 0.0578\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 130us/step - loss: 0.0597\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 129us/step - loss: 0.0578\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0567\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 133us/step - loss: 0.0595\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 135us/step - loss: 0.0587\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 139us/step - loss: 0.0588\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 139us/step - loss: 0.0576\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 138us/step - loss: 0.0570\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 131us/step - loss: 0.0573\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 131us/step - loss: 0.0591\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 131us/step - loss: 0.0584\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 131us/step - loss: 0.0573\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 130us/step - loss: 0.0593\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 130us/step - loss: 0.0570\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 131us/step - loss: 0.0556\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 130us/step - loss: 0.0580\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 130us/step - loss: 0.0566\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 122us/step - loss: 0.0581\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 121us/step - loss: 0.0571\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 25s 123us/step - loss: 0.0573\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 25s 123us/step - loss: 0.0577\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 122us/step - loss: 0.0569\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 121us/step - loss: 0.0566\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 121us/step - loss: 0.0588\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 121us/step - loss: 0.0561\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 129us/step - loss: 0.0566\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 129us/step - loss: 0.0563\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 129us/step - loss: 0.0571\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0581\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 131us/step - loss: 0.0567\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 135us/step - loss: 0.0586\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 135us/step - loss: 0.0569\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 135us/step - loss: 0.0597\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 135us/step - loss: 0.0563\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 135us/step - loss: 0.0576\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 135us/step - loss: 0.0572\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 135us/step - loss: 0.0564\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 129us/step - loss: 0.0540\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0561\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0600\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 119us/step - loss: 0.0566\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0556\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0552\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0559\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0554\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0598\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000/200000 [==============================] - 24s 121us/step - loss: 0.0568\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0559\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0558\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 132us/step - loss: 0.0577\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 138us/step - loss: 0.0568\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 139us/step - loss: 0.0538\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 134us/step - loss: 0.0601\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0573\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0584\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0560\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 25s 127us/step - loss: 0.0543\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 25s 123us/step - loss: 0.0553\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0549\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0555\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0576\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0539\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0565\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0562\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0561\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0563\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0557\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 134us/step - loss: 0.0533\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 135us/step - loss: 0.0546\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 25s 127us/step - loss: 0.0569\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 119us/step - loss: 0.0552\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0566\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0555\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0565\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 25s 126us/step - loss: 0.0546\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0566\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0566\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0544\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0559\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 25s 123us/step - loss: 0.0546\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0566\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0562\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0554\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0558\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0549\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0554\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0525\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 24s 120us/step - loss: 0.0583\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0539\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0562\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0534\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 134us/step - loss: 0.0552\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 135us/step - loss: 0.0553\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 136us/step - loss: 0.0562\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 135us/step - loss: 0.0560\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 135us/step - loss: 0.0558\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 135us/step - loss: 0.0548\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 135us/step - loss: 0.0542\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 135us/step - loss: 0.0565\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 129us/step - loss: 0.0540\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 134us/step - loss: 0.0552\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 134us/step - loss: 0.0534\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 137us/step - loss: 0.0543\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 135us/step - loss: 0.0556\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 134us/step - loss: 0.0547\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 135us/step - loss: 0.0555\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 135us/step - loss: 0.0551\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 26s 128us/step - loss: 0.0565\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 133us/step - loss: 0.0544\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 138us/step - loss: 0.0543\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 136us/step - loss: 0.0542\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 138us/step - loss: 0.0519\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 146us/step - loss: 0.0553\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 146us/step - loss: 0.0549\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 144us/step - loss: 0.0558\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 145us/step - loss: 0.0544\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 145us/step - loss: 0.0572\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 144us/step - loss: 0.0544\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 144us/step - loss: 0.0553\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 145us/step - loss: 0.0550\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 143us/step - loss: 0.0548\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 142us/step - loss: 0.0533\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 142us/step - loss: 0.0576\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 143us/step - loss: 0.0557\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 143us/step - loss: 0.0551\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 143us/step - loss: 0.0535\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 143us/step - loss: 0.0557\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 143us/step - loss: 0.0554\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 143us/step - loss: 0.0533\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 140us/step - loss: 0.0557\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 138us/step - loss: 0.0536\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 138us/step - loss: 0.0551\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 138us/step - loss: 0.0545\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 138us/step - loss: 0.0539\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 137us/step - loss: 0.0548\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 137us/step - loss: 0.0546\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 137us/step - loss: 0.0564\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 137us/step - loss: 0.0544\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000/200000 [==============================] - 27s 137us/step - loss: 0.0555\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 27s 137us/step - loss: 0.0554\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 139us/step - loss: 0.0547\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 143us/step - loss: 0.0544\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 142us/step - loss: 0.0556\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 143us/step - loss: 0.0543\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 142us/step - loss: 0.0530\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 142us/step - loss: 0.0542\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 144us/step - loss: 0.0537\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 145us/step - loss: 0.0567\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 143us/step - loss: 0.0545\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 143us/step - loss: 0.0523\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 28s 142us/step - loss: 0.0536\n",
      "Epoch 1/1\n",
      "200000/200000 [==============================] - 29s 143us/step - loss: 0.0537\n"
     ]
    }
   ],
   "source": [
    "# num_epoch = 10\n",
    "# row_index = np.copy(data_train.user_list)\n",
    "# for epoch in range(num_epoch):\n",
    "#     sw[0].tic()\n",
    "#     np.random.shuffle(row_index)\n",
    "#     sw[0].toc()\n",
    "#     # generate user batches\n",
    "#     for b in utils.batch(row_index, user_batch_size):\n",
    "#         # n_step: number of batch\n",
    "#         n_step += 1\n",
    "#         # prep targets\n",
    "#         # For each user, n_scores_user of the top (defined by the scores of \n",
    "#         # dot product of WMF latent vectors) items are included in the batch\n",
    "#         # Besides that, n_scores_user of random items with their scores\n",
    "#         # (calculated as above) are also included in the batch, which means\n",
    "#         # the ratio is 1:1 for topN and randomN\n",
    "        \n",
    "#         # get top_N items for the users\n",
    "#         # NOTE: if this takes long time, we can preprocess this part of data and store them\n",
    "#         sw[1].tic()\n",
    "#         target_users = np.repeat(b, n_scores_user)\n",
    "#         # target_items = top_k(score_matrix, k = n_scores_user).flatten()\n",
    "#         target_items = topK_matrix[b].flatten()\n",
    "#         score_matrix = np.matmul(u_pref[b, :], v_pref.T)\n",
    "#         sw[1].toc()\n",
    "        \n",
    "#         # get random_N\n",
    "#         sw[2].tic()\n",
    "#         target_users_rand = np.repeat(np.arange(len(b)), n_scores_user)\n",
    "#         target_items_rand = [np.random.choice(v_pref.shape[0], n_scores_user) for _ in b]\n",
    "#         target_items_rand = np.array(target_items_rand).flatten()\n",
    "#         sw[2].toc()\n",
    "        \n",
    "#         sw[3].tic()\n",
    "#         target_scores = score_matrix[target_users_rand, target_items]\n",
    "#         random_scores = score_matrix[target_users_rand, target_items_rand]\n",
    "#         sw[3].toc()\n",
    "\n",
    "#         # merge topN and randomN items per user\n",
    "#         sw[4].tic()\n",
    "#         target_scores = np.append(target_scores, random_scores)\n",
    "#         target_items = np.append(target_items, target_items_rand)\n",
    "#         target_users = np.append(target_users, target_users)\n",
    "#         sw[4].toc()\n",
    "\n",
    "#         # generate content (should before dropout)\n",
    "#         sw[5].tic()\n",
    "#         user_content = generate_user_content(target_users)\n",
    "#         item_content = generate_item_content(target_items)\n",
    "#         sw[5].toc()\n",
    "        \n",
    "#         # dropout process\n",
    "#         sw[6].tic()\n",
    "#         n_targets = len(target_scores)\n",
    "#         # n_targets = min(n_targets, max_data_per_step)\n",
    "#         n_to_drop = int(np.floor(dropout * n_targets))\n",
    "#         user_to_drop = np.random.choice(np.arange(n_targets), size=n_to_drop, replace=False)\n",
    "#         item_to_drop = np.random.choice(np.arange(n_targets), size=n_to_drop, replace=False)\n",
    "#         target_users[user_to_drop] = u_pref_last\n",
    "#         target_items[item_to_drop] = v_pref_last\n",
    "#         sw[6].toc()\n",
    "            \n",
    "#         sw[7].tic()\n",
    "#         loss_out = dropout_net.model.fit(\n",
    "#             x = [u_pref_expanded[target_users, :], v_pref_expanded[target_items, :]] + user_content + item_content,\n",
    "#             y = target_scores, # target\n",
    "#             epochs = 1,\n",
    "#             batch_size = data_batch_size,\n",
    "#             shuffle = True\n",
    "#         )\n",
    "#         sw[7].toc()\n",
    "#     dropout_model_path = '../model/dropout/model_{}.h5'.format(epoch)\n",
    "#     dropout_net.model.save_weights(dropout_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def recall_score_model(klist, data, v_user_all, v_item_all):\n",
    "    recall_at_k = []\n",
    "    max_k = max(klist)\n",
    "    for user in data.user_list:\n",
    "        # get the corresponding embedded vectors\n",
    "        v_user = v_user_all[user]\n",
    "        v_item = v_item_all[data.item_list[user]]\n",
    "        \n",
    "        # compute the scores\n",
    "        score_list = np.matmul(v_user, v_item.T)\n",
    "        score_list = score_list.flatten()\n",
    "        # assert len(score_list) == len(data.item_list[user])\n",
    "        \n",
    "        k = min(max_k, len(data.item_list[user]))\n",
    "        # get the recommended list\n",
    "        indices = top_k(score_list, k)\n",
    "        recommend_list = data.item_list[user][indices]\n",
    "        \n",
    "        # evaluate recall\n",
    "        recall_at_k.append(__recall(klist, data.target_set[user], recommend_list))\n",
    "    return np.mean(recall_at_k, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-01 21:10:54] (start) evaluation of dropout net\n",
      "----------------------------------------\n",
      "../model/dropout/model_0.h5\n",
      "train\n",
      "[ 0.28383819  0.3133038   0.33459792  0.35280115  0.37224075  0.39188641\n",
      "  0.41090602  0.43108989  0.4511209   0.47031803]\n",
      "[ 0.31106296  0.35011179  0.38146012  0.40964138  0.4366865   0.46233263\n",
      "  0.48666695  0.51054204  0.53342427  0.55493316]\n",
      "[ 0.29477555  0.33092184  0.35809482  0.38176983  0.40547192  0.42836127\n",
      "  0.45017148  0.47237953  0.49376972  0.51391898]\n",
      "()\n",
      "test warm\n",
      "[ 0.20321533  0.42469638  0.60585988  0.7350022   0.82007146  0.87812279\n",
      "  0.91584418  0.94080075  0.95760223  0.9684273 ]\n",
      "[ 0.26710343  0.49271289  0.65882621  0.77231052  0.84637798  0.89566094\n",
      "  0.92805651  0.94954364  0.96388396  0.97326312]\n",
      "[ 0.2543738   0.47892089  0.64780681  0.76468168  0.84083147  0.89196753\n",
      "  0.92520767  0.94755884  0.96241878  0.97219976]\n",
      "()\n",
      "test cold user\n",
      "[ 0.5454723   0.6753892   0.75819269  0.81751877  0.86159656  0.89500356\n",
      "  0.92066166  0.94120628  0.9560725   0.96703555]\n",
      "[ 0.54652435  0.67637865  0.75882391  0.81771064  0.86201169  0.89518416\n",
      "  0.92084417  0.94130998  0.95601549  0.96692114]\n",
      "[ 0.54775693  0.67805673  0.76044354  0.81954719  0.86353627  0.89688553\n",
      "  0.92244874  0.94273785  0.95719874  0.96791316]\n",
      "()\n",
      "test cold item\n",
      "[ 0.50677115  0.66512391  0.75257883  0.80990811  0.8483217   0.87681706\n",
      "  0.89738183  0.91296665  0.92514318  0.93597958]\n",
      "[ 0.50933396  0.66704784  0.75334558  0.81032999  0.84887929  0.87725017\n",
      "  0.89779317  0.91337293  0.92534461  0.93613338]\n",
      "[ 0.50951398  0.66674159  0.75341442  0.81018403  0.8487031   0.8771329\n",
      "  0.89759349  0.91317976  0.92535658  0.93619378]\n",
      "()\n",
      "----------------------------------------\n",
      "../model/dropout/model_1.h5\n",
      "train\n",
      "[ 0.28379761  0.3134887   0.33459938  0.35273551  0.37227817  0.39189488\n",
      "  0.41083557  0.43101696  0.45102384  0.4701014 ]\n",
      "[ 0.31106296  0.35011179  0.38146012  0.40964138  0.4366865   0.46233263\n",
      "  0.48666695  0.51054204  0.53342427  0.55493316]\n",
      "[ 0.29545595  0.3317342   0.35941869  0.3831798   0.40707237  0.42996962\n",
      "  0.45173629  0.47379948  0.49506414  0.51522412]\n",
      "()\n",
      "test warm\n",
      "[ 0.20406134  0.42604553  0.60621634  0.73511661  0.81960685  0.87761837\n",
      "  0.91547504  0.94066505  0.95743496  0.968406  ]\n",
      "[ 0.26710343  0.49271289  0.65882621  0.77231052  0.84637798  0.89566094\n",
      "  0.92805651  0.94954364  0.96388396  0.97326312]\n",
      "[ 0.25672275  0.47976432  0.64799397  0.76469262  0.84079737  0.89191853\n",
      "  0.92541463  0.94781809  0.96259855  0.97236142]\n",
      "()\n",
      "test cold user\n",
      "[ 0.54576395  0.67477225  0.75780694  0.81719756  0.86151263  0.89478015\n",
      "  0.92060752  0.94099078  0.95593018  0.9669416 ]\n",
      "[ 0.54652435  0.67637865  0.75882391  0.81771064  0.86201169  0.89518416\n",
      "  0.92084417  0.94130998  0.95601549  0.96692114]\n",
      "[ 0.54304653  0.67241979  0.75609427  0.81583371  0.86066593  0.89450019\n",
      "  0.92054485  0.94113074  0.9559818   0.96698652]\n",
      "()\n",
      "test cold item\n",
      "[ 0.50711824  0.66527273  0.75248118  0.80998528  0.84828559  0.87674504\n",
      "  0.89741418  0.91290349  0.92509279  0.93602547]\n",
      "[ 0.50933396  0.66704784  0.75334558  0.81032999  0.84887929  0.87725017\n",
      "  0.89779317  0.91337293  0.92534461  0.93613338]\n",
      "[ 0.50887701  0.66686536  0.75333196  0.81019429  0.84882188  0.87713974\n",
      "  0.89769709  0.91334921  0.92540474  0.93617031]\n",
      "()\n",
      "----------------------------------------\n",
      "../model/dropout/model_2.h5\n",
      "train\n",
      "[ 0.28366612  0.31339897  0.33467202  0.35285184  0.37220501  0.391814\n",
      "  0.41082971  0.43109469  0.45130416  0.47047269]\n",
      "[ 0.31106296  0.35011179  0.38146012  0.40964138  0.4366865   0.46233263\n",
      "  0.48666695  0.51054204  0.53342427  0.55493316]\n",
      "[ 0.29531095  0.33141526  0.35884675  0.38258978  0.40621007  0.42918346\n",
      "  0.45099883  0.47321703  0.49465693  0.5149159 ]\n",
      "()\n",
      "test warm\n",
      "[ 0.20449935  0.4260954   0.60662786  0.73542622  0.82004919  0.87809624\n",
      "  0.91583137  0.94092155  0.95761567  0.96845258]\n",
      "[ 0.26710343  0.49271289  0.65882621  0.77231052  0.84637798  0.89566094\n",
      "  0.92805651  0.94954364  0.96388396  0.97326312]\n",
      "[ 0.25413874  0.4792323   0.64859514  0.76559131  0.84157     0.89237257\n",
      "  0.92569232  0.94801347  0.96263406  0.97238534]\n",
      "()\n",
      "test cold user\n",
      "[ 0.54556892  0.67558859  0.75796017  0.81704054  0.86128282  0.89470247\n",
      "  0.92063861  0.94118514  0.95615804  0.96706103]\n",
      "[ 0.54652435  0.67637865  0.75882391  0.81771064  0.86201169  0.89518416\n",
      "  0.92084417  0.94130998  0.95601549  0.96692114]\n",
      "[ 0.55253396  0.68321864  0.76567307  0.82376667  0.86682102  0.89949663\n",
      "  0.92457685  0.94414312  0.95825753  0.96874595]\n",
      "()\n",
      "test cold item\n",
      "[ 0.506233    0.66511252  0.75204849  0.80936038  0.84817778  0.8767359\n",
      "  0.89761139  0.91316132  0.92529063  0.93598505]\n",
      "[ 0.50933396  0.66704784  0.75334558  0.81032999  0.84887929  0.87725017\n",
      "  0.89779317  0.91337293  0.92534461  0.93613338]\n",
      "[ 0.5102666   0.66732013  0.75363165  0.81050396  0.84894429  0.87716875\n",
      "  0.89768768  0.91334673  0.92545981  0.93609914]\n",
      "()\n",
      "----------------------------------------\n",
      "../model/dropout/model_3.h5\n",
      "train\n",
      "[ 0.2837453   0.31346533  0.3345655   0.3528422   0.37242654  0.39202743\n",
      "  0.41100765  0.43116047  0.45123534  0.47037452]\n",
      "[ 0.31106296  0.35011179  0.38146012  0.40964138  0.4366865   0.46233263\n",
      "  0.48666695  0.51054204  0.53342427  0.55493316]\n",
      "[ 0.29559559  0.33206537  0.35958721  0.38338603  0.40719552  0.43026983\n",
      "  0.45209953  0.47443201  0.49586952  0.51612389]\n",
      "()\n",
      "test warm\n",
      "[ 0.20324628  0.42548545  0.60587052  0.73478723  0.81967511  0.87808285\n",
      "  0.91565297  0.94093113  0.95768078  0.96846679]\n",
      "[ 0.26710343  0.49271289  0.65882621  0.77231052  0.84637798  0.89566094\n",
      "  0.92805651  0.94954364  0.96388396  0.97326312]\n",
      "[ 0.25570552  0.47983419  0.64824652  0.76558933  0.84140641  0.8922055\n",
      "  0.9256208   0.94795322  0.96264618  0.97232475]\n",
      "()\n",
      "test cold user\n",
      "[ 0.54579946  0.67562589  0.75812739  0.81725045  0.86151027  0.89500423\n",
      "  0.92078036  0.94127464  0.95600026  0.96691189]\n",
      "[ 0.54652435  0.67637865  0.75882391  0.81771064  0.86201169  0.89518416\n",
      "  0.92084417  0.94130998  0.95601549  0.96692114]\n",
      "[ 0.54567544  0.67613233  0.75935543  0.81886704  0.86282491  0.89616314\n",
      "  0.9217974   0.94211172  0.95672544  0.96753376]\n",
      "()\n",
      "test cold item\n",
      "[ 0.50663998  0.66557969  0.75304363  0.81017394  0.8482998   0.87659117\n",
      "  0.89711627  0.91282949  0.92498444  0.93585679]\n",
      "[ 0.50933396  0.66704784  0.75334558  0.81032999  0.84887929  0.87725017\n",
      "  0.89779317  0.91337293  0.92534461  0.93613338]\n",
      "[ 0.50890559  0.66671643  0.75356056  0.81069317  0.848868    0.87712354\n",
      "  0.8975283   0.91323198  0.92531363  0.93607683]\n",
      "()\n",
      "----------------------------------------\n",
      "../model/dropout/model_4.h5\n",
      "train\n",
      "[ 0.28371727  0.31354278  0.33478838  0.35305816  0.37261234  0.39212016\n",
      "  0.4111104   0.43132217  0.45131565  0.47045706]\n",
      "[ 0.31106296  0.35011179  0.38146012  0.40964138  0.4366865   0.46233263\n",
      "  0.48666695  0.51054204  0.53342427  0.55493316]\n",
      "[ 0.29601423  0.33247674  0.35994215  0.38380211  0.40748759  0.43048951\n",
      "  0.45225934  0.4744137   0.49587019  0.51610943]\n",
      "()\n",
      "test warm\n",
      "[ 0.2035161   0.42553726  0.60630558  0.73566468  0.8202458   0.8783457\n",
      "  0.9158146   0.94083715  0.95758406  0.96852023]\n",
      "[ 0.26710343  0.49271289  0.65882621  0.77231052  0.84637798  0.89566094\n",
      "  0.92805651  0.94954364  0.96388396  0.97326312]\n",
      "[ 0.25569558  0.48019748  0.64912468  0.76608425  0.84151124  0.8925066\n",
      "  0.92581288  0.9480881   0.96273109  0.97239705]\n",
      "()\n",
      "test cold user\n",
      "[ 0.54596067  0.6754221   0.75800509  0.81727266  0.86132589  0.89482912\n",
      "  0.92063955  0.94102721  0.95596025  0.96688234]\n",
      "[ 0.54652435  0.67637865  0.75882391  0.81771064  0.86201169  0.89518416\n",
      "  0.92084417  0.94130998  0.95601549  0.96692114]\n",
      "[ 0.55627749  0.6861636   0.76852861  0.82630377  0.86867395  0.90107183\n",
      "  0.92565504  0.94494607  0.95874076  0.96901821]\n",
      "()\n",
      "test cold item\n",
      "[ 0.50713015  0.66525164  0.75223915  0.80934175  0.84811996  0.8768433\n",
      "  0.89747017  0.9130987   0.92531371  0.93610048]\n",
      "[ 0.50933396  0.66704784  0.75334558  0.81032999  0.84887929  0.87725017\n",
      "  0.89779317  0.91337293  0.92534461  0.93613338]\n",
      "[ 0.50893443  0.6669404   0.75348318  0.81042523  0.84905108  0.87725565\n",
      "  0.89765626  0.91319862  0.92530852  0.93617832]\n",
      "()\n",
      "----------------------------------------\n",
      "../model/dropout/model_5.h5\n",
      "train\n",
      "[ 0.28366357  0.31339552  0.33456339  0.35276561  0.3723463   0.39185266\n",
      "  0.41088343  0.43110876  0.45116709  0.47047749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.31106296  0.35011179  0.38146012  0.40964138  0.4366865   0.46233263\n",
      "  0.48666695  0.51054204  0.53342427  0.55493316]\n",
      "[ 0.29644614  0.33303041  0.36046081  0.3841419   0.40782158  0.4309071\n",
      "  0.45262732  0.4747596   0.49607107  0.51624582]\n",
      "()\n",
      "test warm\n",
      "[ 0.20302771  0.42493423  0.60513211  0.73461258  0.81994577  0.87794146\n",
      "  0.91568782  0.9407536   0.95742468  0.96846893]\n",
      "[ 0.26710343  0.49271289  0.65882621  0.77231052  0.84637798  0.89566094\n",
      "  0.92805651  0.94954364  0.96388396  0.97326312]\n",
      "[ 0.25682111  0.48025277  0.64904007  0.76539671  0.84136058  0.89240668\n",
      "  0.92576656  0.94791385  0.96262302  0.97232464]\n",
      "()\n",
      "test cold user\n",
      "[ 0.54650708  0.67584952  0.75796356  0.81729712  0.86164055  0.89506289\n",
      "  0.92083681  0.94136213  0.95606276  0.96701262]\n",
      "[ 0.54652435  0.67637865  0.75882391  0.81771064  0.86201169  0.89518416\n",
      "  0.92084417  0.94130998  0.95601549  0.96692114]\n",
      "[ 0.55424186  0.6847099   0.76703994  0.82509855  0.8677767   0.89994317\n",
      "  0.92470475  0.94430257  0.95826385  0.96886438]\n",
      "()\n",
      "test cold item\n",
      "[ 0.50544912  0.66544753  0.75269967  0.80977502  0.84837083  0.87670607\n",
      "  0.89738504  0.91298509  0.92522989  0.93607623]\n",
      "[ 0.50933396  0.66704784  0.75334558  0.81032999  0.84887929  0.87725017\n",
      "  0.89779317  0.91337293  0.92534461  0.93613338]\n",
      "[ 0.51017077  0.66663382  0.75348446  0.81028027  0.84886703  0.87708564\n",
      "  0.89755437  0.91326621  0.92544776  0.93624926]\n",
      "()\n",
      "----------------------------------------\n",
      "../model/dropout/model_6.h5\n",
      "train\n",
      "[ 0.28371135  0.31341914  0.33456562  0.35270318  0.37226984  0.3917998\n",
      "  0.41079535  0.43109957  0.45113282  0.47029164]\n",
      "[ 0.31106296  0.35011179  0.38146012  0.40964138  0.4366865   0.46233263\n",
      "  0.48666695  0.51054204  0.53342427  0.55493316]\n",
      "[ 0.29612875  0.3326482   0.3603502   0.38422974  0.40794281  0.43089578\n",
      "  0.4527829   0.47492786  0.49634428  0.51646156]\n",
      "()\n",
      "test warm\n",
      "[ 0.20287066  0.4253728   0.60621077  0.73537132  0.82001284  0.87809206\n",
      "  0.91568635  0.94076847  0.9576333   0.96846348]\n",
      "[ 0.26710343  0.49271289  0.65882621  0.77231052  0.84637798  0.89566094\n",
      "  0.92805651  0.94954364  0.96388396  0.97326312]\n",
      "[ 0.25661903  0.48119192  0.64958381  0.76604203  0.84165264  0.8924627\n",
      "  0.92575181  0.94794016  0.96262485  0.97234503]\n",
      "()\n",
      "test cold user\n",
      "[ 0.54641329  0.67566407  0.75822959  0.81727842  0.86164488  0.8949158\n",
      "  0.92098579  0.94116116  0.95598559  0.96687809]\n",
      "[ 0.54652435  0.67637865  0.75882391  0.81771064  0.86201169  0.89518416\n",
      "  0.92084417  0.94130998  0.95601549  0.96692114]\n",
      "[ 0.54572553  0.67560257  0.75873232  0.81826568  0.86255819  0.89604162\n",
      "  0.92176167  0.94210905  0.95684707  0.96764827]\n",
      "()\n",
      "test cold item\n",
      "[ 0.50552651  0.66483848  0.75222849  0.80978049  0.84824311  0.87671129\n",
      "  0.89741903  0.91324267  0.92542904  0.93606847]\n",
      "[ 0.50933396  0.66704784  0.75334558  0.81032999  0.84887929  0.87725017\n",
      "  0.89779317  0.91337293  0.92534461  0.93613338]\n",
      "[ 0.50930685  0.66682718  0.75335588  0.81054687  0.84885841  0.87712851\n",
      "  0.89764395  0.91327887  0.92537815  0.93615293]\n",
      "()\n",
      "----------------------------------------\n",
      "../model/dropout/model_7.h5\n",
      "train\n",
      "[ 0.28392807  0.31340277  0.33469398  0.35295262  0.37253293  0.39201943\n",
      "  0.41089938  0.4310237   0.4510875   0.4703186 ]\n",
      "[ 0.31106296  0.35011179  0.38146012  0.40964138  0.4366865   0.46233263\n",
      "  0.48666695  0.51054204  0.53342427  0.55493316]\n",
      "[ 0.29652352  0.33282777  0.3605738   0.38431333  0.40804863  0.43098784\n",
      "  0.45256713  0.47459947  0.49587076  0.51586041]\n",
      "()\n",
      "test warm\n",
      "[ 0.20226694  0.42550423  0.60590368  0.73482378  0.81982121  0.87809008\n",
      "  0.9156959   0.94090181  0.95753696  0.9684199 ]\n",
      "[ 0.26710343  0.49271289  0.65882621  0.77231052  0.84637798  0.89566094\n",
      "  0.92805651  0.94954364  0.96388396  0.97326312]\n",
      "[ 0.25711788  0.48045572  0.64888399  0.76552378  0.84123907  0.89209393\n",
      "  0.92547673  0.94773942  0.96255448  0.97228612]\n",
      "()\n",
      "test cold user\n",
      "[ 0.54624141  0.67591587  0.7585604   0.81751694  0.86159755  0.89514053\n",
      "  0.92088605  0.9412211   0.95599681  0.9669082 ]\n",
      "[ 0.54652435  0.67637865  0.75882391  0.81771064  0.86201169  0.89518416\n",
      "  0.92084417  0.94130998  0.95601549  0.96692114]\n",
      "[ 0.54811902  0.67944882  0.76247275  0.82103222  0.86441875  0.89767722\n",
      "  0.92286312  0.94290923  0.95729587  0.9679957 ]\n",
      "()\n",
      "test cold item\n",
      "[ 0.50759388  0.66530845  0.75230334  0.80940532  0.84798242  0.87653659\n",
      "  0.89732851  0.91300694  0.92515877  0.9359235 ]\n",
      "[ 0.50933396  0.66704784  0.75334558  0.81032999  0.84887929  0.87725017\n",
      "  0.89779317  0.91337293  0.92534461  0.93613338]\n",
      "[ 0.50987227  0.66734805  0.75386623  0.81059726  0.84894571  0.87720583\n",
      "  0.89786378  0.913389    0.92546384  0.9363045 ]\n",
      "()\n",
      "----------------------------------------\n",
      "../model/dropout/model_8.h5\n",
      "train\n",
      "[ 0.28370463  0.3133864   0.3345476   0.35267482  0.37239075  0.39189048\n",
      "  0.41088031  0.43107565  0.45117514  0.47037983]\n",
      "[ 0.31106296  0.35011179  0.38146012  0.40964138  0.4366865   0.46233263\n",
      "  0.48666695  0.51054204  0.53342427  0.55493316]\n",
      "[ 0.29602374  0.33254962  0.36043504  0.38441063  0.40828617  0.4314884\n",
      "  0.4534454   0.47561948  0.49693868  0.5170971 ]\n",
      "()\n",
      "test warm\n",
      "[ 0.2027103   0.42553438  0.60545021  0.73475177  0.819942    0.87809741\n",
      "  0.9156932   0.94090938  0.95760134  0.96857616]\n",
      "[ 0.26710343  0.49271289  0.65882621  0.77231052  0.84637798  0.89566094\n",
      "  0.92805651  0.94954364  0.96388396  0.97326312]\n",
      "[ 0.25684269  0.48109762  0.64967332  0.7656373   0.84135378  0.89248668\n",
      "  0.92551351  0.94779751  0.96260292  0.97237318]\n",
      "()\n",
      "test cold user\n",
      "[ 0.54554411  0.67545773  0.75786751  0.81702733  0.86110477  0.89470596\n",
      "  0.92069402  0.94116357  0.9560585   0.96703652]\n",
      "[ 0.54652435  0.67637865  0.75882391  0.81771064  0.86201169  0.89518416\n",
      "  0.92084417  0.94130998  0.95601549  0.96692114]\n",
      "[ 0.55372226  0.6835063   0.7655695   0.82339492  0.86631957  0.89876622\n",
      "  0.92368489  0.94339998  0.9577388   0.96839883]\n",
      "()\n",
      "test cold item\n",
      "[ 0.50640695  0.66540853  0.75281545  0.80993466  0.8484814   0.87676265\n",
      "  0.89730343  0.91293614  0.92519878  0.93597746]\n",
      "[ 0.50933396  0.66704784  0.75334558  0.81032999  0.84887929  0.87725017\n",
      "  0.89779317  0.91337293  0.92534461  0.93613338]\n",
      "[ 0.51024791  0.6672879   0.75366391  0.81050831  0.84894655  0.87722283\n",
      "  0.89772463  0.91325603  0.92539506  0.93618394]\n",
      "()\n",
      "----------------------------------------\n",
      "../model/dropout/model_9.h5\n",
      "train\n",
      "[ 0.28385092  0.31333903  0.3346642   0.35286557  0.37231788  0.39182346\n",
      "  0.41078198  0.43099353  0.45114042  0.47031986]\n",
      "[ 0.31106296  0.35011179  0.38146012  0.40964138  0.4366865   0.46233263\n",
      "  0.48666695  0.51054204  0.53342427  0.55493316]\n",
      "[ 0.29600904  0.33281337  0.36056536  0.38435872  0.40801671  0.43098888\n",
      "  0.45269038  0.47472783  0.49601516  0.51612734]\n",
      "()\n",
      "test warm\n",
      "[ 0.20323018  0.42585787  0.60579315  0.73489283  0.81986336  0.87800446\n",
      "  0.9156017   0.94095792  0.95777427  0.96861088]\n",
      "[ 0.26710343  0.49271289  0.65882621  0.77231052  0.84637798  0.89566094\n",
      "  0.92805651  0.94954364  0.96388396  0.97326312]\n",
      "[ 0.25642399  0.48020393  0.64865647  0.76519691  0.84095668  0.89216253\n",
      "  0.92557425  0.94777662  0.96256038  0.97231643]\n",
      "()\n",
      "test cold user\n",
      "[ 0.54588616  0.67549218  0.75808716  0.81752206  0.8616177   0.89495425\n",
      "  0.92066332  0.94114681  0.95602858  0.96696098]\n",
      "[ 0.54652435  0.67637865  0.75882391  0.81771064  0.86201169  0.89518416\n",
      "  0.92084417  0.94130998  0.95601549  0.96692114]\n",
      "[ 0.55464975  0.6843693   0.76660715  0.82426082  0.86723103  0.89954211\n",
      "  0.92424423  0.94387623  0.95797279  0.96852358]\n",
      "()\n",
      "test cold item\n",
      "[ 0.50735073  0.66587336  0.75281591  0.8098706   0.84810806  0.87645652\n",
      "  0.89722224  0.91293611  0.92510601  0.93583212]\n",
      "[ 0.50933396  0.66704784  0.75334558  0.81032999  0.84887929  0.87725017\n",
      "  0.89779317  0.91337293  0.92534461  0.93613338]\n",
      "[ 0.50915089  0.66693998  0.75353027  0.81057831  0.84880986  0.87721726\n",
      "  0.89769607  0.91333775  0.92536938  0.93615095]\n",
      "()\n",
      "[2018-05-01 21:16:01] ( end ) evaluation of dropout net [time elapsed: 0:05:07]\n"
     ]
    }
   ],
   "source": [
    "sub_timer.start('evaluation of dropout net')\n",
    "for model_num in range(10):\n",
    "    model_path = '../model/dropout/exp2/model_%d.h5' % model_num\n",
    "    dropout_net.model.load_weights(model_path)\n",
    "    print('-'*40)\n",
    "    print(model_path)\n",
    "    user_content = generate_user_content(np.arange(num_user))\n",
    "    item_content = generate_item_content(np.arange(num_item))\n",
    "    dropout_user_all = dropout_net.user_model.predict([u_pref_scaled] + user_content)\n",
    "    dropout_item_all = dropout_net.item_model.predict([v_pref_scaled] + item_content)\n",
    "\n",
    "    klist = list(range(5, 51, 5))\n",
    "    print('train')\n",
    "    print(recall_random(klist, data_train))\n",
    "    print(recall_score_model(klist, data_train, v_user_all, v_item_all))\n",
    "    print(recall_score_model(klist, data_train, dropout_user_all, dropout_item_all))\n",
    "    print()\n",
    "\n",
    "    print('test warm')\n",
    "    print(recall_random(klist, data_test_warm))\n",
    "    print(recall_score_model(klist, data_test_warm, v_user_all, v_item_all))\n",
    "    print(recall_score_model(klist, data_test_warm, dropout_user_all, dropout_item_all))\n",
    "    print()\n",
    "    \n",
    "    print('test cold user')\n",
    "    print(recall_random(klist, data_test_cold_user))\n",
    "    print(recall_score_model(klist, data_test_cold_user, v_user_all, v_item_all))\n",
    "    print(recall_score_model(klist, data_test_cold_user, dropout_user_all, dropout_item_all))\n",
    "    print()\n",
    "\n",
    "    print('test cold item')\n",
    "    print(recall_random(klist, data_test_cold_item))\n",
    "    print(recall_score_model(klist, data_test_cold_item, v_user_all, v_item_all))\n",
    "    print(recall_score_model(klist, data_test_cold_item, dropout_user_all, dropout_item_all))\n",
    "    print()\n",
    "sub_timer.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
